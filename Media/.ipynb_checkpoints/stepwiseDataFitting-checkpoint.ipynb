{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b24142ec-d87b-4bca-9d39-4d359840455c",
   "metadata": {},
   "source": [
    "# Stepwise Data Fitting\n",
    "\n",
    "This is an attempt to incrementally analyze the model as we complicate it slowly by adding new dynamics (except for identity which has been left out here). \n",
    "\n",
    "\n",
    "## Models and Simulations\n",
    "Every simulation run has 1000 agents that represent humans, and the richest model has 10 media agents as well as a variable media influence factor. For the first three steps, every parameter combination is run with 30 random initializations. Steps 4 and 5 have been run with 5 random initializations.\n",
    "\n",
    "The following models were tried:\n",
    "\n",
    "1. Simple Hegselmann-Krause with full network.\n",
    "2. Simple Hegselmann-Krause with scale-free network.\n",
    "3. Hegselmann-Krause with normally distributed open-mindedness and scale-free network.\n",
    "3. Hegselmann-Krause with normally distributed open-mindedness, normally distributed initial opinion, and scale-free network.\n",
    "5. Hegselmann-Krause with normally distributed open-mindedness, normally distributed initial opinion, media agents ($MedN = 10$), variable media distribution parameters, variable media influence factor, and scale-free network.\n",
    "6. Hegselmann-Krause with normally distributed open-mindedness, normally distributed initial opinion, media agents ($MedN = 10$), variable media distribution parameters, variable media influence factor, silence mechanisms with memory, and scale-free network.\n",
    "\n",
    "**Notes:** \n",
    "1. The media influence factor  $MedInf \\epsilon [0, 1]$ and is a weighting factor for media-sourced influence. \n",
    "So $MedInf = 0$ implies no influence from media sources, while $MedInf = 1$ implies that media-sourced influence is as strong as other people's opinions.\n",
    "2. The scale-free network parameter is set to 1.\n",
    "3. For Step 4, initial opinion distribution parameters were fixed to values of $\\mu_{InitialPopOp} = 0$ and $\\sigma_{InitialPopOp} = 0.8$ after pilot simulations showed significantly good fits for these values.\n",
    "4. Media opinion positions are drawn from a deterministic set of positions approximating the normal pdf, and have the parameters $\\mu_{MediaOp}$ and $\\sigma_{MediaOp}$.\n",
    "\n",
    "| Model    | Parameters | Random Initializations | Total Simulations |\n",
    "| -------- | ------- | ------- | ------- |\n",
    "| Step 1  | $\\varepsilon$ | 30 | | \n",
    "| Step 2  | $\\varepsilon$ | 30 | | \n",
    "| Step 3  | $\\mu_{\\varepsilon}$, $\\sigma_{\\varepsilon}$| 30 | | \n",
    "| Step 4  | $\\mu_{\\varepsilon}$, $\\sigma_{\\varepsilon}$, $\\sigma_{InitialOp}$| 5 | | \n",
    "| Step 5  | $\\mu_{\\varepsilon}$, $\\sigma_{\\varepsilon}$,  $\\mu_{MediaOp}$, $\\sigma_{MediaOp}$, $MedInf$| 5 | | \n",
    "| Step 6  | $\\mu_{\\varepsilon}$, $\\sigma_{\\varepsilon}$,  $\\mu_{MediaOp}$, $\\sigma_{MediaOp}$, $MedInf$, $Silence_{\\tau}$,  $Silence_{\\delta_{0}}$,  $Silence_{\\alpha}$| 2 | | \n",
    "\n",
    "## Survey Data\n",
    "\n",
    "<>\n",
    "\n",
    "### Parlemeter Data - Initial fitting patterns\n",
    "\n",
    "<>\n",
    "\n",
    "### Dynamic Data - \n",
    "\n",
    "<>\n",
    "\n",
    "## Fitting Strategy\n",
    "\n",
    "<>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b33717f-9d61-4c11-9f7f-c3501840139a",
   "metadata": {},
   "source": [
    "# Preprocessing and Saving Simulation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a77d65-0bcc-466e-bfcc-746b6996fec6",
   "metadata": {},
   "source": [
    "## Globals (SET EACH TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e23037b-9022-45d5-a90f-4427111fb3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_runs_title = \"15.09.25\"\n",
    "\n",
    "# Set below to True if rerunning after saving appropriately in //data//preprocessed//<current_runs_title>\n",
    "load_preprocessed_data = True\n",
    "\n",
    "# Set below to True if there is a need to preprocess data (such as while running for the first time for new data)\n",
    "preprocess_data = True\n",
    "\n",
    "# Set below to True if running for the first time or need to preprocess and save for some other reason\n",
    "save_preprocessed_data = True\n",
    "\n",
    "\n",
    "# set a smaller processing unit if it benefits you, else keep it to range(1, 5)\n",
    "steps_to_process =  range(1, 7)\n",
    "\n",
    "# For the folder names for each step\n",
    "step_input_folder_name = []\n",
    "step_output_folder_name = []\n",
    "\n",
    "# Specific sub-folder names (folder name within data/cluster/endsim)\n",
    "step_input_folder_name.append(\"Step1\") \n",
    "step_input_folder_name.append(\"Step2\") \n",
    "step_input_folder_name.append(\"Step3\") \n",
    "step_input_folder_name.append(\"Step4_5reps\") \n",
    "step_input_folder_name.append(\"Step5_5reps\") \n",
    "step_input_folder_name.append(\"Step6_2reps\") \n",
    "\n",
    "step_output_folder_name = step_input_folder_name\n",
    "\n",
    "step_titles = [\"Simple HK with Full Network\", \"Simple HK with Scale-Free Network\", \"Heterogenous Openness\", \"Heterogenous Openness with normal opinion distributions\", \"Media and Influence\", \"Media with Silence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cfaabfd-0b4d-4b34-b958-9afea2878611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import jensenshannon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e5b323-a936-4157-81a2-acb1cada7f9a",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b39b9808-b30f-4db0-a713-2e1adfa166f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the preprocessed data will be saved\n",
    "full_preprocessed_data_folder_path = os.path.join(\"data\\\\preprocessed\\\\\" + current_runs_title)\n",
    "\n",
    "\n",
    "# Wheer the output plots will be saved\n",
    "plots_folder_path =  os.path.join(\"analysis\\\\plots\\\\\" + current_runs_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc69555-be5b-47dd-b210-bd30b92515c6",
   "metadata": {},
   "source": [
    "## Issue - Step 4 has a LOT OF redundancies and no media\n",
    "\n",
    "I accidentally ran this step with 0 media agents, so we have a great number of repeat simulations and no media in this step.\n",
    "We will rectify this by adding a step 5 with media and rerunning the simulations with the new setting.\n",
    "\n",
    "However, for the time being we need to remove the extra simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83b39c82-ad13-411a-9548-16aa782f71d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Define the regex pattern to extract parameters\n",
    "# pattern = r\"EndSim_epsM(-?[\\d\\.]+)_epsSD(-?[\\d\\.]+)___OpD([a-zA-Z-]+)_OpM(-?[\\d\\.]+)_OpSD(-?[\\d\\.]+)___Net([a-zA-Z-]+)___NAgents(\\d+)___RS(\\d+)__MedInfF([\\d\\.]+)___MedD([a-zA-Z-]+)_MedN(\\d+)_MedM(-?[\\d\\.]+)_MedSD(-?[\\d\\.]+)\"\n",
    "\n",
    "# def extract_eps_params(filename):\n",
    "#     \"\"\"Extract epsM and epsSD parameters from filename\"\"\"\n",
    "#     base_name = os.path.splitext(filename)[0]\n",
    "#     match = re.search(pattern, base_name)\n",
    "#     if match:\n",
    "#         epsM = float(match.group(1))\n",
    "#         epsSD = float(match.group(2))\n",
    "#         RS = int(match.group(8))\n",
    "#         return (epsM, epsSD, RS)\n",
    "#     return None\n",
    "    \n",
    "# # Alternative: Move files instead of deleting (safer approach)\n",
    "# def move_redundant_files(directory, backup_dir=\"redundant_backup\"):\n",
    "#     \"\"\"Move redundant files to a backup directory instead of deleting\"\"\"\n",
    "#     # Create backup directory if it doesn't exist\n",
    "#     backup_path = os.path.join(directory, backup_dir)\n",
    "#     os.makedirs(backup_path, exist_ok=True)\n",
    "    \n",
    "#     # Get all CSV files\n",
    "#     csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "    \n",
    "#     unique_combinations = {}\n",
    "#     files_to_keep = []\n",
    "#     files_to_move = []\n",
    "    \n",
    "#     # Process each file\n",
    "#     for filename in csv_files:\n",
    "#         params = extract_eps_params(filename)\n",
    "#         if params:\n",
    "#             epsM, epsSD, RS = params\n",
    "#             combination = (epsM, epsSD, RS)\n",
    "            \n",
    "#             if combination not in unique_combinations:\n",
    "#                 unique_combinations[combination] = filename\n",
    "#                 files_to_keep.append(filename)\n",
    "#             else:\n",
    "#                 files_to_move.append(filename)\n",
    "#         else:\n",
    "#             print(f\"Warning: Could not parse parameters from {filename}\")\n",
    "    \n",
    "#     # Move redundant files\n",
    "#     moved_count = 0\n",
    "#     for filename in files_to_move:\n",
    "#         src_path = os.path.join(directory, filename)\n",
    "#         dest_path = os.path.join(backup_path, filename)\n",
    "#         try:\n",
    "#             os.rename(src_path, dest_path)\n",
    "#             moved_count += 1\n",
    "#             # print(f\"Moved to backup: {filename}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error moving {filename}: {e}\")\n",
    "    \n",
    "#     # Print summary\n",
    "#     print(f\"\\nSummary:\")\n",
    "#     print(f\"Total files processed: {len(csv_files)}\")\n",
    "#     print(f\"Unique (epsM, epsSD, RS) combinations: {len(unique_combinations)}\")\n",
    "#     print(f\"Files kept: {len(files_to_keep)}\")\n",
    "#     print(f\"Files moved to backup: {moved_count}\")\n",
    "    \n",
    "#     return files_to_keep, files_to_move\n",
    "\n",
    "\n",
    "# move_redundant_files(\"data/cluster/endsim/step4_5reps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5ab314-f43b-4283-8500-b4bb1fa046e9",
   "metadata": {},
   "source": [
    "##  Reading and Preprocessing Simulation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b9ae1d2-7810-496c-a9d6-1cf9be0a248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex pattern for all files\n",
    "pattern = (\n",
    "    r\"EndSim_epsM(-?[\\d\\.]+)_epsSD(-?[\\d\\.]+)\"\n",
    "    r\"___OpD([a-zA-Z-]+)_OpM(-?[\\d\\.]+)_OpSD(-?[\\d\\.]+)\"\n",
    "    r\"___Net([a-zA-Z-]+)___NAgents(\\d+)___RS(\\d+)\"\n",
    "    r\"__MedInfF([\\d\\.]+)___MedD([a-zA-Z-]+)_MedN(\\d+)_MedM(-?[\\d\\.]+)_MedSD(-?[\\d\\.]+)\"\n",
    "    r\"(_SA(-?[\\d\\.]+)_ST(-?[\\d\\.]+)_SDO(-?[\\d\\.]+)_SBB([a-zA-Z]+))?\"\n",
    ")\n",
    "\n",
    "def parse_filename(filename):\n",
    "    \"\"\"Extract parameters from filename using regex with optional silence parameters\"\"\"\n",
    "    # Remove .csv extension before parsing\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "    match = re.search(pattern, base_name)\n",
    "    \n",
    "    if match:\n",
    "        result = {\n",
    "            # 'filename': filename,\n",
    "            'epsM': float(match.group(1)),\n",
    "            'epsSD': float(match.group(2)),\n",
    "            'OpD': match.group(3),\n",
    "            'OpM': float(match.group(4)),\n",
    "            'OpSD': float(match.group(5)),\n",
    "            'NetworkType': match.group(6),\n",
    "            'NAgents': int(match.group(7)),\n",
    "            'RandomSeed': int(match.group(8)),\n",
    "            'MedInfF': float(match.group(9)),\n",
    "            'MedD': match.group(10),\n",
    "            'MedN': int(match.group(11)),\n",
    "            'MedM': float(match.group(12)),\n",
    "            'MedSD': float(match.group(13))\n",
    "        }\n",
    "        # Check if silence parameters are present\n",
    "        if match.group(14):  # The entire silence block exists\n",
    "            # silence_parsed_counts = silence_parsed_counts + 1 # increment the silence parsed counter\n",
    "            result.update({\n",
    "                'Silence_Alpha': float(match.group(15)),\n",
    "                'Silence_Tau': float(match.group(16)),\n",
    "                'Silence_Delta0': float(match.group(17)),\n",
    "                'SilenceByBoundary': match.group(18).lower() == 'true'\n",
    "            })\n",
    "        else:\n",
    "            # no_silence_parsed_counts = no_silence_parsed_counts + 1\n",
    "            # Set default values for silence parameters if not present\n",
    "            result.update({\n",
    "                'Silence_Alpha': None,\n",
    "                'Silence_Tau': None,\n",
    "                'Silence_Delta0': None,\n",
    "                'SilenceByBoundary': None\n",
    "            })\n",
    "        \n",
    "        return result\n",
    "    return None\n",
    "    \n",
    "def extract_opinion_sections(content):\n",
    "    \"\"\"Extract media and individual opinion sections from file content\"\"\"\n",
    "    # Split content into sections using separator lines\n",
    "    sections = re.split(r'-{10,}', content)\n",
    "    \n",
    "    # We expect at least 4 sections:\n",
    "    # 0: Metadata\n",
    "    # 1: Network connections (skip)\n",
    "    # 2: Media opinions\n",
    "    # 3: Individual opinions\n",
    "    if len(sections) < 4:\n",
    "        return None, None\n",
    "    \n",
    "    # Extract media opinions section\n",
    "    media_section = sections[2].strip()\n",
    "    # Extract individual opinions section\n",
    "    individual_section = sections[3].strip()\n",
    "    \n",
    "    return media_section, individual_section\n",
    "\n",
    "def parse_opinion_section(section):\n",
    "    \"\"\"Parse a single opinion section into a DataFrame\"\"\"\n",
    "    if not section:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Clean section by removing any empty lines and brackets\n",
    "    cleaned_lines = []\n",
    "    for line in section.split('\\n'):\n",
    "        if line.strip() and not line.startswith('[') and not line.startswith('--'):\n",
    "            cleaned_lines.append(line)\n",
    "    \n",
    "    if not cleaned_lines:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Recreate CSV content\n",
    "    csv_content = \"\\n\".join(cleaned_lines)\n",
    "    \n",
    "    # Parse as CSV\n",
    "    try:\n",
    "        return pd.read_csv(StringIO(csv_content))\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def process_simulation_file(filepath):\n",
    "    \"\"\"Process a single simulation file with section handling\"\"\"\n",
    "    # Extract parameters from filename\n",
    "    filename = os.path.basename(filepath)\n",
    "    params = parse_filename(filename)\n",
    "    if not params:\n",
    "        print(f\"Skipping {filename} - pattern not matched\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Read entire file content\n",
    "        with open(filepath, 'r') as f:\n",
    "            content = f.read()\n",
    "        # Extract opinion sections\n",
    "        media_section, individual_section = extract_opinion_sections(content)\n",
    "\n",
    "        # Parse media opinions\n",
    "        media_df = parse_opinion_section(media_section)\n",
    "\n",
    "        media_opinions = []\n",
    "        # print(media_df.columns)\n",
    "\n",
    "        if not media_df.empty and ' opinion)' in media_df:\n",
    "            media_opinions = media_df[' opinion)'].apply(\n",
    "                lambda x: float(str(x).strip('[]'))).values\n",
    "        \n",
    "        # Parse individual opinions\n",
    "        individual_df = parse_opinion_section(individual_section)\n",
    "        individual_opinions = []\n",
    "        if not individual_df.empty and 'opinion' in individual_df:\n",
    "            individual_opinions = individual_df['opinion'].apply(\n",
    "                lambda x: float(str(x).strip('[]'))).values\n",
    "        \n",
    "        \n",
    "        individual_initial_opinions = []\n",
    "        if not individual_df.empty and 'initialOpinion' in individual_df:\n",
    "            individual_initial_opinions = individual_df['initialOpinion'].apply(\n",
    "                lambda x: float(str(x).strip('[]'))).values\n",
    "        \n",
    "        # Add to params\n",
    "        params['individual_opinions'] = np.array(individual_opinions)\n",
    "        params['individual_initial_opinions'] = np.array(individual_initial_opinions)\n",
    "        params['media_opinions'] = np.array(media_opinions)\n",
    "        \n",
    "        # Add static parameters\n",
    "        params['NAgents'] = 1000\n",
    "        params['MedN'] = 20\n",
    "        params['NetType'] = \"scale-free\"\n",
    "        params['ScaleFreeDegree'] = 1\n",
    "        \n",
    "        return params\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_all_simulations(directory):\n",
    "    \"\"\"Process all simulation files in a directory\"\"\"\n",
    "    all_data = []\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.csv'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            result = process_simulation_file(filepath)\n",
    "            if result:\n",
    "                all_data.append(result)\n",
    "                processed_count += 1\n",
    "            else:\n",
    "                skipped_count += 1\n",
    "    print(f\"\\n\\nSummary: Processed={processed_count}, Skipped={skipped_count}\")\n",
    "    return pd.DataFrame(all_data) if all_data else pd.DataFrame()\n",
    "\n",
    "def get_simulations_for_step(stepNo):\n",
    "    simulations_df = load_all_simulations(\"data\\\\cluster\\\\endsim\\\\\" + step_input_folder_name[stepNo - 1])\n",
    "    \n",
    "    if not simulations_df.empty:\n",
    "        print(f\"\\nSuccessfully loaded {len(simulations_df)} simulations for Step {stepNo}\")\n",
    "        print(\"Parameter ranges:\")\n",
    "        for param in ['epsM', 'epsSD', 'OpM', 'OpSD', 'MedM', 'MedSD', 'MedInfF']:\n",
    "            values = simulations_df[param]\n",
    "            print(f\"{param}: min={values.min()}, max={values.max()}\")\n",
    "        \n",
    "        # Show example opinion data\n",
    "        sample = simulations_df.iloc[0]\n",
    "        print(f\"\\nSample simulation opinions:\")\n",
    "        print(f\"Individual opinions: {len(sample['individual_opinions'])} values\")\n",
    "        print(f\"Media opinions: {len(sample['media_opinions'])} values\")\n",
    "    else:\n",
    "        print(f\"No simulations loaded for Step {stepNo}. Please check file patterns and data quality.\")\n",
    "\n",
    "    return(simulations_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc800754-60a5-4c4f-bad6-ec947166e056",
   "metadata": {},
   "source": [
    "## Loading up Simulation Data into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bee8fa58-0987-4d79-9235-49c605ed2356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Summary: Processed=2100, Skipped=0\n",
      "\n",
      "Successfully loaded 2100 simulations for Step 1\n",
      "Parameter ranges:\n",
      "epsM: min=0.01, max=0.7\n",
      "epsSD: min=0.0, max=0.0\n",
      "OpM: min=0.0, max=0.0\n",
      "OpSD: min=0.49, max=0.49\n",
      "MedM: min=0.0, max=0.0\n",
      "MedSD: min=0.79, max=0.79\n",
      "MedInfF: min=0.91, max=0.91\n",
      "\n",
      "Sample simulation opinions:\n",
      "Individual opinions: 1000 values\n",
      "Media opinions: 0 values\n",
      "\n",
      "\n",
      "Summary: Processed=2100, Skipped=0\n",
      "\n",
      "Successfully loaded 2100 simulations for Step 2\n",
      "Parameter ranges:\n",
      "epsM: min=0.01, max=0.7\n",
      "epsSD: min=0.0, max=0.0\n",
      "OpM: min=0.0, max=0.0\n",
      "OpSD: min=0.49, max=0.49\n",
      "MedM: min=0.0, max=0.0\n",
      "MedSD: min=0.79, max=0.79\n",
      "MedInfF: min=0.91, max=0.91\n",
      "\n",
      "Sample simulation opinions:\n",
      "Individual opinions: 1000 values\n",
      "Media opinions: 0 values\n",
      "\n",
      "\n",
      "Summary: Processed=3000, Skipped=0\n",
      "\n",
      "Successfully loaded 3000 simulations for Step 3\n",
      "Parameter ranges:\n",
      "epsM: min=0.01, max=0.25\n",
      "epsSD: min=0.0, max=0.15\n",
      "OpM: min=0.0, max=0.0\n",
      "OpSD: min=0.49, max=0.49\n",
      "MedM: min=0.0, max=0.0\n",
      "MedSD: min=0.79, max=0.79\n",
      "MedInfF: min=0.91, max=0.91\n",
      "\n",
      "Sample simulation opinions:\n",
      "Individual opinions: 1000 values\n",
      "Media opinions: 0 values\n",
      "\n",
      "\n",
      "Summary: Processed=2160, Skipped=0\n",
      "\n",
      "Successfully loaded 2160 simulations for Step 4\n",
      "Parameter ranges:\n",
      "epsM: min=0.01, max=0.35\n",
      "epsSD: min=0.0, max=0.15\n",
      "OpM: min=0.0, max=0.0\n",
      "OpSD: min=0.0, max=1.0\n",
      "MedM: min=0.0, max=0.0\n",
      "MedSD: min=0.3, max=0.3\n",
      "MedInfF: min=0.5, max=0.5\n",
      "\n",
      "Sample simulation opinions:\n",
      "Individual opinions: 1000 values\n",
      "Media opinions: 0 values\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m(preprocess_data):\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Running all sims and storing them as DF's\u001b[39;00m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m stepNo \u001b[38;5;129;01min\u001b[39;00m steps_to_process:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m         simulation_df_step.append(\u001b[43mget_simulations_for_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstepNo\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 147\u001b[39m, in \u001b[36mget_simulations_for_step\u001b[39m\u001b[34m(stepNo)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_simulations_for_step\u001b[39m(stepNo):\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     simulations_df = \u001b[43mload_all_simulations\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mcluster\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mendsim\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_input_folder_name\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstepNo\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m simulations_df.empty:\n\u001b[32m    150\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSuccessfully loaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(simulations_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m simulations for Step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstepNo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 137\u001b[39m, in \u001b[36mload_all_simulations\u001b[39m\u001b[34m(directory)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filename.endswith(\u001b[33m'\u001b[39m\u001b[33m.csv\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    136\u001b[39m     filepath = os.path.join(directory, filename)\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     result = \u001b[43mprocess_simulation_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[32m    139\u001b[39m         all_data.append(result)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 82\u001b[39m, in \u001b[36mprocess_simulation_file\u001b[39m\u001b[34m(filepath)\u001b[39m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     81\u001b[39m     \u001b[38;5;66;03m# Read entire file content\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     83\u001b[39m         content = f.read()\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# Extract opinion sections\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:263\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, errors)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# To Load simulation data\n",
    "simulation_df_step = []    # The main list of Dataframes in which we will be loading the data\n",
    "\n",
    "if(preprocess_data):\n",
    "    # Running all sims and storing them as DF's\n",
    "    for stepNo in steps_to_process:\n",
    "        simulation_df_step.append(get_simulations_for_step(stepNo))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dce8da1-c172-454c-b90b-b2dc0940a53f",
   "metadata": {},
   "source": [
    "## Saving Preprocessed Simulation Dataframes as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ace620a-6e0d-4e53-8f45-0c00b732a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(save_preprocessed_data):\n",
    "    if not os.path.exists(full_preprocessed_data_folder_path):\n",
    "        os.makedirs(full_preprocessed_data_folder_path)\n",
    "    for stepNo in steps_to_process:\n",
    "        file_name = f\"simulated_data_Step{stepNo}.csv\"\n",
    "        file_path = os.path.join(full_preprocessed_data_folder_path, file_name)\n",
    "        simulation_df_step[stepNo - 1].to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4d8b51-2b8c-4ea0-988c-69d9bf6e03fd",
   "metadata": {},
   "source": [
    "## Loading Preprocessed Simulation Dataframes from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd2080b-b6e7-4017-b309-cd868a80ca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(load_preprocessed_data):\n",
    "    del simulation_df_step\n",
    "    simulation_df_step = [] # Don't do this unless we are loading subsequently, otherwise we will have an emptied list and much sadness to go with.\n",
    "    for stepNo in steps_to_process:\n",
    "        file_name = f\"simulated_data_Step{stepNo}.csv\"\n",
    "        file_path = os.path.join(full_preprocessed_data_folder_path, file_name)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Need to convert the arrays to numpy arrays\n",
    "        individual_opinions = []\n",
    "        if not df.empty and 'individual_opinions' in df:\n",
    "            df['individual_opinions'] = df['individual_opinions'].apply(\n",
    "                lambda x: np.fromstring(x.strip('[]').replace('\\n', ' '), sep=' ', dtype=float))\n",
    "        \n",
    "        individual_initial_opinions = []\n",
    "        if not df.empty and 'individual_initial_opinions' in df:\n",
    "            df['individual_initial_opinions'] = df['individual_initial_opinions'].apply(\n",
    "                lambda x: np.fromstring(x.strip('[]').replace('\\n', ' '), sep=' ', dtype=float))\n",
    "        \n",
    "        simulation_df_step.append(df) # append to the stepwise dataframe list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04a2b91-c8f6-4040-b909-6f39ba90c54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_df_step[3]['media_opinions']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57ec621-4725-40b1-b5c3-b2a8cba06279",
   "metadata": {},
   "source": [
    "## For Binning Simulation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a41a7e-4f12-409d-a30e-4380d0347055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bin edges for 10-point scale mapping\n",
    "bin_edges = np.array([-1.0, -0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "\n",
    "# Create bin interval labels for display\n",
    "bin_intervals = [\n",
    "    \"[-1.0, -0.8)\",\n",
    "    \"[-0.8, -0.6)\",\n",
    "    \"[-0.6, -0.4)\",\n",
    "    \"[-0.4, -0.2)\",\n",
    "    \"[-0.2, 0.0)\",\n",
    "    \"[0.0, 0.2)\",\n",
    "    \"[0.2, 0.4)\",\n",
    "    \"[0.4, 0.6)\",\n",
    "    \"[0.6, 0.8)\",\n",
    "    \"[0.8, 1.0]\"\n",
    "]\n",
    "\n",
    "def bin_opinions(opinions):\n",
    "    \"\"\"Bin opinions into 10-point scale categories\"\"\"\n",
    "    # Digitize opinions (returns bin indices 1-10)\n",
    "    bin_indices = np.digitize(opinions, bin_edges[1:-1], right=False)\n",
    "    return bin_indices\n",
    "\n",
    "def bin_distribution(opinions):\n",
    "    \"\"\"Compute normalized distribution across bins\"\"\"\n",
    "    counts, _ = np.histogram(opinions, bins=bin_edges)\n",
    "    return counts / counts.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c21053-1ae5-4cca-b9bb-ef393c80faaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding binned columns to DF\n",
    "\n",
    "for stepNo in steps_to_process:\n",
    "    simulations_df = simulation_df_step[stepNo - 1]\n",
    "    \n",
    "    # Add binned opinions and distributions to DataFrame\n",
    "    simulations_df['binned_opinions'] = simulations_df['individual_opinions'].apply(\n",
    "        lambda x: bin_opinions(np.array(x))\n",
    "    )\n",
    "    \n",
    "    simulations_df['binned_distribution'] = simulations_df['individual_opinions'].apply(\n",
    "        lambda x: bin_distribution(np.array(x))\n",
    "    )\n",
    "    \n",
    "    # Add bin interval labels as a constant column\n",
    "    simulations_df['bin_intervals'] = [bin_intervals] * len(simulations_df)\n",
    "\n",
    "    # Club the two central bins to account for anchoring in the real data\n",
    "    simulations_df['binned_distributions_5_6clubbed'] = simulations_df['binned_distribution'].apply(\n",
    "        lambda dist: np.concatenate([dist[:4], [dist[4] + dist[5]], dist[6:]])\n",
    "    )\n",
    "\n",
    "    # Repeating the same for the initial opinion distribution\n",
    "\n",
    "    # Add binned opinions and distributions to DataFrame\n",
    "    simulations_df['binned_initial_opinions'] = simulations_df['individual_initial_opinions'].apply(\n",
    "        lambda x: bin_opinions(np.array(x))\n",
    "    )\n",
    "    \n",
    "    simulations_df['binned_initial_distribution'] = simulations_df['individual_initial_opinions'].apply(\n",
    "        lambda x: bin_distribution(np.array(x))\n",
    "    )\n",
    "\n",
    "    # Club the two central bins to account for anchoring in the real data\n",
    "    simulations_df['binned_initial_distributions_5_6clubbed'] = simulations_df['binned_initial_distribution'].apply(\n",
    "        lambda dist: np.concatenate([dist[:4], [dist[4] + dist[5]], dist[6:]])\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "    # Display the updated DataFrame\n",
    "    # print(\"\\nDataFrame with binned opinions:\")\n",
    "    # print(simulations_df[['epsM', 'epsSD', 'OpM', 'OpSD', 'MedM', 'MedSD', \n",
    "    #                       'binned_opinions', 'binned_distribution', 'bin_intervals']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab151fd-3462-4dd9-bbab-71228675a0da",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61328aa6-9bc9-4ab7-afd6-f91285b83599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a Simulation Index column to the dataframes\n",
    "\n",
    "\n",
    "# Initialize cumulative index counter\n",
    "cumulative_index = 0\n",
    "\n",
    "\n",
    "for stepNo in steps_to_process:\n",
    "    # Get the current DataFrame\n",
    "    simulations_df = simulation_df_step[stepNo - 1]\n",
    "    \n",
    "    # Add model simulation index (starts at 1 for each DataFrame)\n",
    "    simulations_df['model_simulation_index'] = range(1, len(simulations_df) + 1)\n",
    "    \n",
    "    # Add cumulative simulation index (continues from previous DataFrame)\n",
    "    simulations_df['cumulative_simulation_index'] = range(cumulative_index + 1, \n",
    "                                                         cumulative_index + len(simulations_df) + 1)\n",
    "    \n",
    "    # Update cumulative index for next DataFrame\n",
    "    cumulative_index += len(simulations_df)\n",
    "    \n",
    "    # Update the DataFrame in the list\n",
    "    simulation_df_step[stepNo - 1] = simulations_df    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64872d6-76ac-4a54-9a87-5bfb7b1bb1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for stepNo in steps_to_process:\n",
    "    simulations_df = simulation_df_step[stepNo - 1]\n",
    "    print(f\"Step {stepNo}:\")\n",
    "    print(f\"  Model indices: {simulations_df['model_simulation_index'].min()} to {simulations_df['model_simulation_index'].max()}\")\n",
    "    print(f\"  Cumulative indices: {simulations_df['cumulative_simulation_index'].min()} to {simulations_df['cumulative_simulation_index'].max()}\")\n",
    "    print(f\"  Number of simulations: {len(simulations_df)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7ffb16-9592-4221-8d57-8a8257dfecfc",
   "metadata": {},
   "source": [
    "## Pooled Histograms for each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d34be07-9e2c-4176-ab9a-98bc6bee48f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stepNo in steps_to_process:\n",
    "    simulations_df = simulation_df_step[stepNo - 1]\n",
    "    # Create pooled histogram of all opinions\n",
    "    all_opinions = np.concatenate(simulations_df['individual_opinions'].values)\n",
    "    pooled_counts, _ = np.histogram(all_opinions, bins=bin_edges)\n",
    "    pooled_distribution = pooled_counts / pooled_counts.sum()\n",
    "    \n",
    "    # Plot the pooled histogram\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(1, 11), pooled_distribution, color='skyblue', edgecolor='black')\n",
    "    plt.xticks(range(1, 11), bin_intervals, rotation=45, ha='right')\n",
    "    plt.xlabel('Opinion Bins')\n",
    "    plt.ylabel('Proportion of Opinions')\n",
    "    plt.title('Pooled Opinion Distribution Across All Simulations in ' +  step_titles[stepNo - 1] + \" (Step \" +  str(stepNo)+ \")\")\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pooled_opinion_distribution.png', dpi=300)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef2fd40-1483-4816-931d-2fb91f8c2cd0",
   "metadata": {},
   "source": [
    "## Generating Individual histograms for specific simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eccc6a-5a51-45a5-93aa-d6194efc0348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Define bin edges and labels\n",
    "bin_edges = np.array([-1.0, -0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "bin_intervals = [\n",
    "    \"[-1.0, -0.8)\", \"[-0.8, -0.6)\", \"[-0.6, -0.4)\", \"[-0.4, -0.2)\", \n",
    "    \"[-0.2, 0.0)\", \"[0.0, 0.2)\", \"[0.2, 0.4)\", \"[0.4, 0.6)\", \n",
    "    \"[0.6, 0.8)\", \"[0.8, 1.0]\"\n",
    "]\n",
    "\n",
    "def save_simulation_histogram(row, index, stepNum, histograms_save_path):\n",
    "    # histograms_save_path = os.join(histograms_save_path, step_output_folder_name[stepNum - 1])\n",
    "    \"\"\"Create and save histogram for a single simulation with both initial and final opinions\"\"\"\n",
    "    # Extract parameters for title and filename based on step number\n",
    "    if stepNum == 1 or stepNum == 2:\n",
    "        # Only epsM for steps 1 and 2\n",
    "        filename = f\"sim_{index:04d}_epsM{row['epsM']:.2f}.png\".replace(\".\", \"dot\")\n",
    "        title = f\"Opinion Distribution (Step {stepNum}): εμ={row['epsM']:.2f}\"\n",
    "    elif stepNum == 3:\n",
    "        # epsM and epsSD for step 3\n",
    "        filename = f\"sim_{index:04d}_epsM{row['epsM']:.2f}_epsSD{row['epsSD']:.2f}.png\".replace(\".\", \"dot\")\n",
    "        title = f\"Opinion Distribution (Step {stepNum}): εμ={row['epsM']:.2f}, εσ={row['epsSD']:.2f}\"\n",
    "    else:\n",
    "        # All parameters for step 4 and beyond\n",
    "        filename = (\n",
    "            f\"sim_{index:04d}_\"\n",
    "            f\"epsM{row['epsM']:.2f}_epsSD{row['epsSD']:.2f}_\"\n",
    "            f\"OpM{row['OpM']:.2f}_OpSD{row['OpSD']:.2f}_\"\n",
    "            f\"MedM{row['MedM']:.2f}_MedSD{row['MedSD']:.2f}__\"\n",
    "            f\"MedInf{row['MedInfF']:.2f}.png\"\n",
    "        ).replace(\".\", \"dot\")\n",
    "        title = (\n",
    "            f\"Opinion Distribution (Step {stepNum}): \"\n",
    "            f\"εμ={row['epsM']:.2f}, εσ={row['epsSD']:.2f}, \"\n",
    "            f\"Medμ={row['MedM']:.2f}, Medσ={row['MedSD']:.2f}, \"\n",
    "            f\"MedInfF={row['MedInfF']:.2f}\"\n",
    "        )\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    # Plot histograms for both initial and final opinions\n",
    "    n_initial, _, _ = plt.hist(\n",
    "        row['individual_initial_opinions'],\n",
    "        bins=bin_edges,\n",
    "        density=False,\n",
    "        alpha=0.5,\n",
    "        color='steelblue',\n",
    "        edgecolor='black',\n",
    "        label='Initial Opinions'\n",
    "    )\n",
    "    \n",
    "    n_final, bins, patches = plt.hist(\n",
    "        row['individual_opinions'], \n",
    "        bins=bin_edges, \n",
    "        density=False, \n",
    "        alpha=0.7, \n",
    "        color='green',\n",
    "        edgecolor='black',\n",
    "        label='Final Opinions'\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "    # Add vertical lines at media positions for reference\n",
    "    if(stepNo == 5):\n",
    "        for medop in row['media_opinions']:\n",
    "                plt.axvline(medop, color='red', linestyle='--', alpha=0.5, label='Media Opinion' if medop == row['media_opinions'][0] else \"\")\n",
    "    \n",
    "    # Format plot\n",
    "    plt.title(title, fontsize=14, pad=20)\n",
    "    plt.xlabel('Opinion Value', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.xticks(bin_edges, rotation=45)\n",
    "    plt.xlim(-1.05, 1.05)\n",
    "    # plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Add bin labels\n",
    "    bin_centers = 0.5 * (bin_edges[1:] + bin_edges[:-1])\n",
    "    # for i in range(len(bin_intervals)):\n",
    "    #     plt.text(\n",
    "    #         bin_centers[i], max(n_final[i], n_initial[i]) + 0.01, \n",
    "    #         f\"F:{n_final[i]}\\nI:{n_initial[i]}\\n{bin_intervals[i]}\", \n",
    "    #         ha='center', va='bottom', fontsize=8\n",
    "    #     )\n",
    "    \n",
    "    # Add statistics box\n",
    "    stats_text = (\n",
    "        f\"N: {len(row['individual_opinions'])}\\n\"\n",
    "        f\"Initial Mean: {np.mean(row['individual_initial_opinions']):.3f}\\n\"\n",
    "        f\"Final Mean: {np.mean(row['individual_opinions']):.3f}\\n\"\n",
    "        f\"Change: {np.mean(row['individual_opinions']) - np.mean(row['individual_initial_opinions']):.3f}\"\n",
    "    )\n",
    "    \n",
    "    plt.gcf().text(0.50, 0.85, stats_text, fontsize=10, \n",
    "                   bbox=dict(facecolor='white', alpha=0.5))\n",
    "    \n",
    "    # Save and close\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(histograms_save_path, filename), dpi=150)\n",
    "    plt.close()\n",
    "    return filename\n",
    "\n",
    "# Function to generate histogram by cumulative index\n",
    "def generate_histogram_by_cumulative_index(cumulative_index, histograms_save_path):\n",
    "    \"\"\"Generate histogram for a simulation based on its cumulative index\"\"\"\n",
    "    for stepNo in steps_to_process:\n",
    "        simulations_df = simulation_df_step[stepNo - 1]\n",
    "        if cumulative_index in simulations_df['cumulative_simulation_index'].values:\n",
    "            row = simulations_df[simulations_df['cumulative_simulation_index'] == cumulative_index].iloc[0]\n",
    "            model_index = row['model_simulation_index']\n",
    "            return save_simulation_histogram(row, cumulative_index, stepNo, histograms_save_path)\n",
    "    print(f\"Error: Cumulative index {cumulative_index} not found.\")\n",
    "    return None\n",
    "\n",
    "# Function to generate histogram by step and model index\n",
    "def generate_histogram_by_step_model_index(stepNo, model_index, histograms_save_path):\n",
    "    \"\"\"Generate histogram for a simulation based on step number and model index\"\"\"\n",
    "    if stepNo > len(simulation_df_step):\n",
    "        print(f\"Error: Step number {stepNo} is out of range.\")\n",
    "        return None\n",
    "    \n",
    "    simulations_df = simulation_df_step[stepNo - 1]\n",
    "    if model_index not in simulations_df['model_simulation_index'].values:\n",
    "        print(f\"Error: Model index {model_index} not found in step {stepNo}.\")\n",
    "        return None\n",
    "    \n",
    "    row = simulations_df[simulations_df['model_simulation_index'] == model_index].iloc[0]\n",
    "    cumulative_index = row['cumulative_simulation_index']\n",
    "    return save_simulation_histogram(row, cumulative_index, stepNo, histograms_save_path)\n",
    "\n",
    "# Setup directories for each step\n",
    "for stepNo in steps_to_process:\n",
    "    # Setup Path\n",
    "    histograms_save_path = os.path.join(plots_folder_path, \"selected_sims_histograms\")\n",
    "    if not os.path.exists(histograms_save_path):\n",
    "        os.makedirs(histograms_save_path)\n",
    "    \n",
    "    # You can now call either:\n",
    "    # generate_histogram_by_cumulative_index(1, histograms_save_path)\n",
    "    # or\n",
    "    generate_histogram_by_step_model_index(4, 1, histograms_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda98fab-7ada-4c21-9f77-6925c334cce0",
   "metadata": {},
   "source": [
    "# Load Survey Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597ce113-3675-4090-bafe-90e5e3c91d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "folder_path = \"data/EU_dataset\"\n",
    "file_title = \"Cleaned_Parlemeter_Data-LeftRight\"\n",
    "file_name = file_title + \".xlsx\"\n",
    "survey_file = os.path.join(folder_path, file_name)\n",
    "\n",
    "sheets = pd.ExcelFile(survey_file).sheet_names[1:]  # Skip 'RowHeaders'\n",
    "\n",
    "survey_data = []\n",
    "\n",
    "for sheet in sheets:\n",
    "    country = sheet.split(\"-\")[0]  # Extract country code (e.g., \"BE\")\n",
    "    df_sheet = pd.read_excel(survey_file, sheet_name=sheet, header=None)\n",
    "    \n",
    "    # Years are in the first row\n",
    "    years = df_sheet.iloc[0, :].tolist()\n",
    "    \n",
    "    # Extract counts for positions 1-10 (skip % rows and DK/Refusal)\n",
    "    counts_matrix = []\n",
    "    for i in range(10):  # Positions 1-10\n",
    "        row_index = 1 + 2 * i  # Count rows are at indices 1,3,5,...,19\n",
    "        counts_matrix.append(df_sheet.iloc[row_index, :].tolist())\n",
    "    \n",
    "    # Process each year\n",
    "    for j, year in enumerate(years):\n",
    "        counts = [float(counts_matrix[i][j]) for i in range(10)]\n",
    "        total_valid = sum(counts)\n",
    "        \n",
    "        # Normalized distribution (1-10)\n",
    "        distribution = [c / total_valid for c in counts]\n",
    "\n",
    "        # Pool bins 5 and 6\n",
    "        pooled_counts = counts[:4] + [counts[4] + counts[5]] + counts[6:]\n",
    "        pooled_distribution = [c / total_valid for c in pooled_counts]\n",
    "        \n",
    "        survey_data.append({\n",
    "            \"country\": country,\n",
    "            \"year\": int(year),\n",
    "            \"binned_distribution\": distribution,\n",
    "            \"binned_distribution_5_6clubbed\": pooled_distribution,\n",
    "            \"total_respondents\": total_valid\n",
    "        })\n",
    "survey_df = pd.DataFrame(survey_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75acdfe3-0637-4acf-bd35-87fc5a5523b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simulation_df_step[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5bedba-105d-4e63-9772-7e28a496b195",
   "metadata": {},
   "source": [
    "## Fitting Data for all Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc617a1-5ebc-4598-aec9-93ebd978642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure vectors are numpy arrays of floats\n",
    "def to_array(x):\n",
    "    return np.array(x, dtype=float)\n",
    "\n",
    "# List of dataframes for the fits (now combined initial and final)\n",
    "jsfit_dfs_stepwise = []\n",
    "best_jsfits_stepwise = []\n",
    "\n",
    "for stepNo in steps_to_process:\n",
    "    simulations_df = simulation_df_step[stepNo - 1]\n",
    "    this_fit_data = []\n",
    "    \n",
    "    # Precompute survey distributions once\n",
    "    survey_dists = []\n",
    "    for _, surv_row in survey_df.iterrows():\n",
    "        survey_dists.append({\n",
    "            'country': surv_row['country'],\n",
    "            'year': surv_row['year'],\n",
    "            'distribution': to_array(surv_row['binned_distribution_5_6clubbed'])\n",
    "        })\n",
    "    \n",
    "    for sim_idx, sim_row in simulations_df.iterrows():\n",
    "        sim_dist_final = to_array(sim_row['binned_distributions_5_6clubbed'])\n",
    "        sim_dist_initial = to_array(sim_row['binned_initial_distributions_5_6clubbed'])\n",
    "        # print(sim_row)\n",
    "        # Extract simulation parameters once\n",
    "        sim_params = {\n",
    "            'cum_sim_index': sim_row['cumulative_simulation_index'],\n",
    "            'epsM': sim_row['epsM'],\n",
    "            'epsSD': sim_row['epsSD'],\n",
    "            'OpM': sim_row['OpM'],\n",
    "            'OpSD': sim_row['OpSD'],\n",
    "            'MedM': sim_row['MedM'],\n",
    "            'RS': sim_row['RandomSeed'],\n",
    "            'MedSD': sim_row['MedSD'],\n",
    "            'MedInfF': sim_row['MedInfF']\n",
    "        }\n",
    "        \n",
    "        for surv_data in survey_dists:\n",
    "            surv_dist = surv_data['distribution']\n",
    "            \n",
    "            # Jensen–Shannon divergence for both initial and final\n",
    "            jsd_final = jensenshannon(sim_dist_final, surv_dist)\n",
    "            jsd_initial = jensenshannon(sim_dist_initial, surv_dist)\n",
    "            \n",
    "            # Create a single entry with both distances\n",
    "            this_fit_data.append({\n",
    "                **sim_params,  # Unpack simulation parameters\n",
    "                'country': surv_data['country'],\n",
    "                'year': surv_data['year'],\n",
    "                'distance_final': jsd_final,\n",
    "                'distance_initial': jsd_initial,\n",
    "                'distance_delta': jsd_final - jsd_initial\n",
    "            })\n",
    "    \n",
    "    # Create a single DataFrame for this step\n",
    "    thisfit_df = pd.DataFrame(this_fit_data)\n",
    "    jsfit_dfs_stepwise.append(thisfit_df)\n",
    "    \n",
    "    # Extract best fits for both initial and final\n",
    "    best_fit_final = thisfit_df.sort_values('distance_final').groupby(['country', 'year']).head(5)\n",
    "    best_fit_initial = thisfit_df.sort_values('distance_initial').groupby(['country', 'year']).head(5)\n",
    "    \n",
    "    # Combine best fits into a single DataFrame with a type indicator\n",
    "    best_fit_final['distance_type'] = 'final'\n",
    "    best_fit_initial['distance_type'] = 'initial'\n",
    "    \n",
    "    best_fit_combined = pd.concat([best_fit_final, best_fit_initial], ignore_index=True)\n",
    "    best_jsfits_stepwise.append(best_fit_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed39dc62-68d6-4892-a9df-6470afbc143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsfit_dfs_stepwise[1].nlargest(30, 'distance_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3423dfb-e8fe-42bd-8c65-8706df24ccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "step1fits=jsfit_dfs_stepwise[0]\n",
    "step1fits[step1fits['epsM']==0.21]['distance_final'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a332f71-11d4-4a9f-b6e4-10c3e96e6b61",
   "metadata": {},
   "source": [
    "## Plotting JSD v/s Epsilon\n",
    "\n",
    "### Steps 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92c814d-382e-441d-937d-66de58f690ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fit_threshold = 0.05 # Set this to filter the best fits\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "# Create output directory for plots\n",
    "plot_output_dir = os.path.join(plots_folder_path, \"Fit_Plots\")\n",
    "os.makedirs(plot_output_dir, exist_ok=True)\n",
    "\n",
    "# Process steps 1 and 2 (index 0 and 1 in our list)\n",
    "for step_idx in steps_to_process:\n",
    "    stepNo = step_idx + 1  # Convert to 1-based indexing\n",
    "    fit_df = jsfit_dfs_stepwise[step_idx]\n",
    "    \n",
    "    print(f\"Processing Step {stepNo} with {len(fit_df)} data points\")\n",
    "    \n",
    "    # Plot 1: Final distance against epsM\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Group by epsM and calculate mean and standard error\n",
    "    grouped = fit_df.groupby('epsM')['distance_final'].agg(['mean', 'std', 'count'])\n",
    "    grouped['se'] = grouped['std'] / np.sqrt(grouped['count'])\n",
    "\n",
    "    \n",
    "    grouped_initial = fit_df.groupby('epsM')['distance_initial'].agg(['mean', 'std', 'count'])\n",
    "    grouped_initial['se']  =  grouped_initial['std'] / np.sqrt(grouped_initial['count'])\n",
    "\n",
    "    \n",
    "    plt.errorbar(grouped.index, grouped['mean'], yerr=grouped['se'], \n",
    "                 fmt='o-', capsize=5, capthick=2, linewidth=2, markersize=8, label=\"End of Sim divergence\")\n",
    "\n",
    "    #Initial for comparison\n",
    "    plt.errorbar(grouped_initial.index, grouped_initial['mean'], yerr=grouped_initial['se'], \n",
    "                 fmt='o-', capsize=5, capthick=2, linewidth=2, markersize=8, label=\"Beginning of Sim divergence\")\n",
    "    \n",
    "    plt.xlabel('ε (Openness)', fontsize=14)\n",
    "    plt.ylabel('End of Sim Jensen-Shannon Divergence', fontsize=14)\n",
    "    plt.title(f'Final Fit Quality vs Openness (ε)\\n{step_titles[stepNo - 1]}', fontsize=16)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(os.path.join(plot_output_dir, f'step{stepNo}_final_vs_epsM.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 2: Final distance against epsM (only improved fits)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Filter for improved fits (final distance < initial distance)\n",
    "    improved_fits = fit_df[fit_df['distance_delta'] < 0]\n",
    "    \n",
    "    if len(improved_fits) > 0:\n",
    "        # Group by epsM and calculate statistics\n",
    "        grouped_improved = improved_fits.groupby('epsM')['distance_final'].agg(['mean', 'std', 'count'])\n",
    "        grouped_improved['se'] = grouped_improved['std'] / np.sqrt(grouped_improved['count'])\n",
    "        \n",
    "        plt.errorbar(grouped_improved.index, grouped_improved['mean'], yerr=grouped_improved['se'], \n",
    "                     fmt='s-', capsize=5, capthick=2, linewidth=2, markersize=8, color='green', label = \"Improved fits only\")\n",
    "        \n",
    "        # Add original for comparison (light gray)\n",
    "        plt.errorbar(grouped.index, grouped['mean'], yerr=grouped['se'], \n",
    "                     fmt='o--', capsize=3, capthick=1, linewidth=1, markersize=5, \n",
    "                     color='gray', alpha=0.5, label='All simulations')\n",
    "        \n",
    "        plt.xlabel('ε (Openness)', fontsize=14)\n",
    "        plt.ylabel('End of Sim Jensen-Shannon Divergence', fontsize=14)\n",
    "        plt.title(f'Improved Fits Only (Final < Initial)\\n{step_titles[stepNo - 1]}', fontsize=16)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(plot_output_dir, f'step{stepNo}_improved_final_vs_epsM.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(f\"No improved fits found for Step {stepNo}\")\n",
    "        plt.close()\n",
    "    \n",
    "    # Plot 3: Delta distance against epsM\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Group by epsM and calculate mean and standard error for delta\n",
    "    grouped_delta = fit_df.groupby('epsM')['distance_delta'].agg(['mean', 'std', 'count'])\n",
    "    grouped_delta['se'] = grouped_delta['std'] / np.sqrt(grouped_delta['count'])\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.errorbar(grouped_delta.index, grouped_delta['mean'], yerr=grouped_delta['se'], \n",
    "                 fmt='o-', capsize=5, capthick=2, linewidth=2, markersize=8, color='purple')\n",
    "    \n",
    "    # Add horizontal line at zero for reference\n",
    "    plt.axhline(y=0, color='red', linestyle='--', alpha=0.7, linewidth=1)\n",
    "    \n",
    "    plt.xlabel('ε (Openness)', fontsize=14)\n",
    "    plt.ylabel('Δ Divergence (Final - Initial)', fontsize=14)\n",
    "    plt.title(f'Fit Improvement vs ε (Negative values = Improvement)\\n{step_titles[stepNo - 1]}', fontsize=16)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_output_dir, f'step{stepNo}_delta_vs_epsM.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    # Plot 4: Only the best of the best fits.\n",
    "    best_fits = fit_df[fit_df['distance_final'] < best_fit_threshold]\n",
    "    print(\"Best fits dataframe: \\n\")\n",
    "    print(best_fits)\n",
    "    if len(best_fits) > 0:\n",
    "        # Group by epsM and calculate statistics\n",
    "        grouped_best = best_fits.groupby('epsM')['distance_final'].agg(['mean', 'std', 'count'])\n",
    "        grouped_best['se'] = grouped_best['std'] / np.sqrt(grouped_best['count'])\n",
    "\n",
    "        \n",
    "        grouped_best_initial = best_fits.groupby('epsM')['distance_initial'].agg(['mean', 'std', 'count'])\n",
    "        grouped_best_initial['se'] = grouped_best_initial['std'] / np.sqrt(grouped_best_initial['count'])\n",
    "        \n",
    "        plt.errorbar(grouped_best.index, grouped_best['mean'], yerr=grouped_best['se'], \n",
    "                     fmt='s-', capsize=5, capthick=2, linewidth=2, markersize=8, color='green', label = \"End of Sim Divergence\")\n",
    "        \n",
    "        # Initial for comparison\n",
    "        plt.errorbar(grouped_best.index, grouped_best['mean'], yerr=grouped_best['se'], \n",
    "                 fmt='o-', capsize=5, capthick=2, linewidth=2, markersize=8, label=\"Beginning of Sim divergence\")\n",
    "        \n",
    "        plt.xlabel('ε (Openness)', fontsize=14)\n",
    "        plt.ylabel('End of Sim Jensen-Shannon Divergence', fontsize=14)\n",
    "        plt.title(f'Best Fits Only (JS < {best_fit_threshold})\\n{step_titles[stepNo - 1]}', fontsize=16)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(plot_output_dir, f'step{stepNo}_best_final_vs_epsM.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(f\"No good fits found for Step {stepNo}\")\n",
    "        plt.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Additional analysis: Print summary statistics\n",
    "    print(f\"\\nStep {stepNo} Summary Statistics:\")\n",
    "    print(f\"Total simulations: {fit_df['cum_sim_index'].nunique()}\")\n",
    "    print(f\"Total country-year combinations: {fit_df[['country', 'year']].drop_duplicates().shape[0]}\")\n",
    "    print(f\"Mean final distance: {fit_df['distance_final'].mean():.4f}\")\n",
    "    print(f\"Mean initial distance: {fit_df['distance_initial'].mean():.4f}\")\n",
    "    print(f\"Mean delta: {fit_df['distance_delta'].mean():.4f}\")\n",
    "    print(f\"Percentage improved: {(len(improved_fits) / len(fit_df) * 100):.1f}%\")\n",
    "    \n",
    "    # Best epsM values\n",
    "    best_epsM_final = grouped.sort_values('mean').index[0]\n",
    "    best_epsM_delta = grouped_delta.sort_values('mean').index[0]\n",
    "    print(f\"Best epsM for final distance: {best_epsM_final:.3f}\")\n",
    "    print(f\"Best epsM for improvement: {best_epsM_delta:.3f}\")\n",
    "\n",
    "print(f\"\\nAll plots saved to: {plot_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ec44b9-6431-4fe8-be4b-20803cda7d79",
   "metadata": {},
   "source": [
    "## Color Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bf708c-c8a0-4704-b1f1-7f432076cc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create output directory for plots\n",
    "plot_output_dir = os.path.join(plots_folder_path, \"Colorplots\")\n",
    "best_fit_threshold = 0.05\n",
    "\n",
    "# Greek symbols for parameter names\n",
    "param_labels = {\n",
    "    'epsM': r'$\\mu_{\\varepsilon}$',\n",
    "    'epsSD': r'$\\sigma_{\\varepsilon}$', \n",
    "    'OpSD': r'$\\sigma_{InitialOp}$',\n",
    "    'MedM': r'$\\mu_{MediaOp}$',\n",
    "    'MedSD': r'$\\sigma_{MediaOp}$',\n",
    "    'MedInfF': 'Media Influence'\n",
    "}\n",
    "\n",
    "# Global style settings\n",
    "plt.rcParams.update({\n",
    "    'font.size': 14,\n",
    "    'axes.titlesize': 16,\n",
    "    'axes.labelsize': 14,\n",
    "    'xtick.labelsize': 8,\n",
    "    'ytick.labelsize': 12,\n",
    "    'legend.fontsize': 12,\n",
    "    'figure.titlesize': 18\n",
    "})\n",
    "\n",
    "# Global flags\n",
    "SHOW_GRID = True  # Set to True to enable grid\n",
    "\n",
    "# Common color normalization for all plots across all steps\n",
    "def get_global_color_norm(variant, all_steps_data):\n",
    "    \"\"\"Get common color normalization across all steps for a variant\"\"\"\n",
    "    all_values = []\n",
    "    \n",
    "    for step_data in all_steps_data:\n",
    "        if step_data is not None and len(step_data) > 0:\n",
    "            if variant == 'mean_jsd':\n",
    "                values = step_data.groupby(['epsM', 'epsSD'])['distance_final'].mean()\n",
    "            elif variant == 'mean_jsd_improved':\n",
    "                improved = step_data[step_data['distance_delta'] < 0]\n",
    "                values = improved.groupby(['epsM', 'epsSD'])['distance_final'].mean()\n",
    "            elif variant == 'mean_jsd_best_fit':\n",
    "                best_fit = step_data[step_data['distance_final'] < best_fit_threshold]\n",
    "                values = best_fit.groupby(['epsM', 'epsSD'])['distance_final'].mean()\n",
    "            elif variant == 'best_jsd':\n",
    "                values = step_data.groupby(['epsM', 'epsSD'])['distance_final'].min()\n",
    "            \n",
    "            if len(values) > 0:\n",
    "                all_values.extend(values.values)\n",
    "    \n",
    "    if not all_values:\n",
    "        return colors.Normalize(vmin=0, vmax=1)\n",
    "    \n",
    "    return colors.Normalize(vmin=min(all_values), vmax=max(all_values))\n",
    "\n",
    "# Function to create categorical heatmap\n",
    "def create_categorical_heatmap(pivot_table, xlabel, ylabel, title, norm, cmap='viridis_r', ax=None):\n",
    "    \"\"\"Create a categorical heatmap with proper discrete parameter handling\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Get the unique parameter values\n",
    "    x_categories = pivot_table.columns.tolist()\n",
    "    y_categories = pivot_table.index.tolist()\n",
    "    \n",
    "    # Create the heatmap\n",
    "    im = ax.imshow(pivot_table.values, origin='lower', aspect='auto',\n",
    "                   norm=norm, cmap=cmap)\n",
    "    \n",
    "    # Set ticks and labels at cell centers\n",
    "    ax.set_xticks(np.arange(len(x_categories)))\n",
    "    ax.set_yticks(np.arange(len(y_categories)))\n",
    "    ax.set_xticklabels([f\"{x:.2f}\" for x in x_categories])\n",
    "    ax.set_yticklabels([f\"{y:.2f}\" for y in y_categories])\n",
    "    \n",
    "    # Add proper grid lines if enabled\n",
    "    if SHOW_GRID:\n",
    "        # Add grid lines at cell boundaries\n",
    "        ax.set_xticks(np.arange(len(x_categories) + 1) - 0.5, minor=True)\n",
    "        ax.set_yticks(np.arange(len(y_categories) + 1) - 0.5, minor=True)\n",
    "        ax.grid(which=\"minor\", color=\"white\", linestyle='-', linewidth=1.5, alpha=0.8)\n",
    "        ax.tick_params(which=\"minor\", length=0)\n",
    "    \n",
    "    ax.set_xlabel(xlabel, fontsize=14)\n",
    "    ax.set_ylabel(ylabel, fontsize=14)\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    \n",
    "    return im\n",
    "\n",
    "# Function to process data for a variant\n",
    "def process_variant_data(df, variant):\n",
    "    \"\"\"Process data for different variants\"\"\"\n",
    "    if variant == 'mean_jsd':\n",
    "        return df.groupby(['epsM', 'epsSD'])['distance_final'].mean().reset_index()\n",
    "    elif variant == 'mean_jsd_improved':\n",
    "        improved = df[df['distance_delta'] < 0]\n",
    "        return improved.groupby(['epsM', 'epsSD'])['distance_final'].mean().reset_index()\n",
    "    elif variant == 'mean_jsd_best_fit':\n",
    "        best_fit = df[df['distance_final'] < best_fit_threshold]\n",
    "        return best_fit.groupby(['epsM', 'epsSD'])['distance_final'].mean().reset_index()\n",
    "    elif variant == 'best_jsd':\n",
    "        return df.groupby(['epsM', 'epsSD'])['distance_final'].min().reset_index()\n",
    "\n",
    "# Collect all data for global normalization\n",
    "all_steps_data = jsfit_dfs_stepwise[2:5]  # Steps 3, 4, 5\n",
    "\n",
    "# Precompute global norms for each variant\n",
    "global_norms = {}\n",
    "variants = ['mean_jsd', 'mean_jsd_improved', 'mean_jsd_best_fit', 'best_jsd']\n",
    "\n",
    "for variant in variants:\n",
    "    global_norms[variant] = get_global_color_norm(variant, all_steps_data)\n",
    "    print(f\"{variant}: vmin={global_norms[variant].vmin:.4f}, vmax={global_norms[variant].vmax:.4f}\")\n",
    "\n",
    "# Step 3: epsM and epsSD only\n",
    "print(\"Processing Step 3...\")\n",
    "step3_dir = os.path.join(plot_output_dir, \"step3\")\n",
    "os.makedirs(step3_dir, exist_ok=True)\n",
    "\n",
    "step3_data = jsfit_dfs_stepwise[2] if len(jsfit_dfs_stepwise) > 2 else None\n",
    "\n",
    "if step3_data is not None:\n",
    "    # Get unique parameter values for the entire step\n",
    "    unique_epsM = sorted(step3_data['epsM'].unique())\n",
    "    unique_epsSD = sorted(step3_data['epsSD'].unique())\n",
    "    \n",
    "    for variant in variants:\n",
    "        variant_data = process_variant_data(step3_data, variant)\n",
    "        \n",
    "        if len(variant_data) > 0:\n",
    "            # Create pivot table and ensure all parameter combinations are represented\n",
    "            pivot = variant_data.pivot_table(index='epsSD', columns='epsM', values='distance_final')\n",
    "            pivot = pivot.reindex(index=unique_epsSD, columns=unique_epsM)\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(12, 9))\n",
    "            norm = global_norms[variant]\n",
    "            \n",
    "            im = create_categorical_heatmap(pivot, param_labels['epsM'], param_labels['epsSD'],\n",
    "                                          f'Step 3: {variant.replace(\"_\", \" \").title()}', norm, ax=ax)\n",
    "            \n",
    "            # Add colorbar\n",
    "            cbar = plt.colorbar(im, ax=ax)\n",
    "            cbar.set_label('Jensen-Shannon Distance', fontsize=14)\n",
    "            \n",
    "            # Save best_jsd directly in step folder, others in subfolders\n",
    "            if variant == 'best_jsd':\n",
    "                plt.savefig(os.path.join(step3_dir, f'step3_{variant}.png'), \n",
    "                           dpi=300, bbox_inches='tight')\n",
    "            else:\n",
    "                variant_dir = os.path.join(step3_dir, variant)\n",
    "                os.makedirs(variant_dir, exist_ok=True)\n",
    "                plt.savefig(os.path.join(variant_dir, f'step3_{variant}.png'), \n",
    "                           dpi=300, bbox_inches='tight')\n",
    "            \n",
    "            plt.close()\n",
    "\n",
    "# Step 4: epsM, epsSD, and OpSD\n",
    "print(\"Processing Step 4...\")\n",
    "step4_dir = os.path.join(plot_output_dir, \"step4\")\n",
    "os.makedirs(step4_dir, exist_ok=True)\n",
    "\n",
    "step4_data = jsfit_dfs_stepwise[3] if len(jsfit_dfs_stepwise) > 3 else None\n",
    "\n",
    "if step4_data is not None:\n",
    "    # Get unique parameter values for the entire step\n",
    "    unique_epsM = sorted(step4_data['epsM'].unique())\n",
    "    unique_epsSD = sorted(step4_data['epsSD'].unique())\n",
    "    unique_OpSD = sorted(step4_data['OpSD'].unique())\n",
    "    \n",
    "    for variant in variants:\n",
    "        for OpSD_val in unique_OpSD:\n",
    "            OpSD_data = step4_data[step4_data['OpSD'] == OpSD_val]\n",
    "            variant_data = process_variant_data(OpSD_data, variant)\n",
    "            \n",
    "            if len(variant_data) > 0:\n",
    "                # Create pivot table and ensure all parameter combinations are represented\n",
    "                pivot = variant_data.pivot_table(index='epsSD', columns='epsM', values='distance_final')\n",
    "                pivot = pivot.reindex(index=unique_epsSD, columns=unique_epsM)\n",
    "                \n",
    "                fig, ax = plt.subplots(figsize=(12, 9))\n",
    "                norm = global_norms[variant]\n",
    "                \n",
    "                im = create_categorical_heatmap(pivot, param_labels['epsM'], param_labels['epsSD'],\n",
    "                                              f'Step 4: {variant.replace(\"_\", \" \").title()}\\n{param_labels[\"OpSD\"]} = {OpSD_val:.2f}',\n",
    "                                              norm, ax=ax)\n",
    "                \n",
    "                # Add colorbar\n",
    "                cbar = plt.colorbar(im, ax=ax)\n",
    "                cbar.set_label('Jensen-Shannon Distance', fontsize=14)\n",
    "                \n",
    "                # Save best_jsd directly in step folder, others in subfolders\n",
    "                if variant == 'best_jsd':\n",
    "                    plt.savefig(os.path.join(step4_dir, f'step4_OpSD_{OpSD_val:.2f}_{variant}.png'), \n",
    "                               dpi=300, bbox_inches='tight')\n",
    "                else:\n",
    "                    variant_dir = os.path.join(step4_dir, variant)\n",
    "                    os.makedirs(variant_dir, exist_ok=True)\n",
    "                    plt.savefig(os.path.join(variant_dir, f'step4_OpSD_{OpSD_val:.2f}_{variant}.png'), \n",
    "                               dpi=300, bbox_inches='tight')\n",
    "                \n",
    "                plt.close()\n",
    "\n",
    "# Step 5: Full parameter space\n",
    "print(\"Processing Step 5...\")\n",
    "step5_dir = os.path.join(plot_output_dir, \"step5\")\n",
    "os.makedirs(step5_dir, exist_ok=True)\n",
    "\n",
    "step5_data = jsfit_dfs_stepwise[4] if len(jsfit_dfs_stepwise) > 4 else None\n",
    "\n",
    "if step5_data is not None:\n",
    "    # Get unique parameter values for the entire step\n",
    "    unique_epsM = sorted(step5_data['epsM'].unique())\n",
    "    unique_epsSD = sorted(step5_data['epsSD'].unique())\n",
    "    unique_MedM = sorted(step5_data['MedM'].unique())\n",
    "    unique_MedSD = sorted(step5_data['MedSD'].unique())\n",
    "    unique_MedInfF = sorted(step5_data['MedInfF'].unique())\n",
    "    \n",
    "    for variant in variants:\n",
    "        for MedInfF_val in unique_MedInfF:\n",
    "            MedInfF_data = step5_data[step5_data['MedInfF'] == MedInfF_val]\n",
    "            \n",
    "            # Create subplot grid\n",
    "            fig, axes = plt.subplots(len(unique_MedSD), len(unique_MedM), \n",
    "                                    figsize=(6*len(unique_MedM), 5*len(unique_MedSD)),\n",
    "                                    sharex=True, sharey=True)\n",
    "            \n",
    "            if len(unique_MedSD) == 1 and len(unique_MedM) == 1:\n",
    "                axes = np.array([[axes]])\n",
    "            elif len(unique_MedSD) == 1:\n",
    "                axes = axes[np.newaxis, :]\n",
    "            elif len(unique_MedM) == 1:\n",
    "                axes = axes[:, np.newaxis]\n",
    "            \n",
    "            # Get global norm for this variant\n",
    "            norm = global_norms[variant]\n",
    "            \n",
    "            # Find the first valid subplot to create colorbar\n",
    "            first_valid_im = None\n",
    "            \n",
    "            for i, MedSD_val in enumerate(unique_MedSD):\n",
    "                for j, MedM_val in enumerate(unique_MedM):\n",
    "                    ax = axes[i, j]\n",
    "                    \n",
    "                    # Filter data for this parameter combination\n",
    "                    param_data = MedInfF_data[\n",
    "                        (MedInfF_data['MedSD'] == MedSD_val) & \n",
    "                        (MedInfF_data['MedM'] == MedM_val)\n",
    "                    ]\n",
    "                    \n",
    "                    if len(param_data) > 0:\n",
    "                        variant_data = process_variant_data(param_data, variant)\n",
    "                        if len(variant_data) > 0:\n",
    "                            # Create pivot table and ensure all parameter combinations are represented\n",
    "                            pivot = variant_data.pivot_table(index='epsSD', columns='epsM', values='distance_final')\n",
    "                            pivot = pivot.reindex(index=unique_epsSD, columns=unique_epsM)\n",
    "                            \n",
    "                            im = create_categorical_heatmap(pivot, '', '', '', norm, ax=ax)\n",
    "                            if first_valid_im is None:\n",
    "                                first_valid_im = im\n",
    "                            \n",
    "                            # Add parameter labels to subplots\n",
    "                            if i == len(unique_MedSD) - 1:\n",
    "                                ax.set_xlabel(param_labels['epsM'], fontsize=12)\n",
    "                            if j == 0:\n",
    "                                ax.set_ylabel(param_labels['epsSD'], fontsize=12)\n",
    "                            \n",
    "                            # Add MedM and MedSD values to subplot title\n",
    "                            ax.set_title(f'{param_labels[\"MedM\"]}={MedM_val:.2f}\\n{param_labels[\"MedSD\"]}={MedSD_val:.2f}', \n",
    "                                       fontsize=11)\n",
    "                    else:\n",
    "                        ax.axis('off')\n",
    "            \n",
    "            # Add overall title\n",
    "            fig.suptitle(f'Step 5: {variant.replace(\"_\", \" \").title()}\\n'\n",
    "                        f'{param_labels[\"MedInfF\"]} = {MedInfF_val:.2f}', \n",
    "                        fontsize=18, y=0.95)\n",
    "            \n",
    "            # Add single colorbar for the entire figure\n",
    "            if first_valid_im is not None:\n",
    "                cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "                cbar = fig.colorbar(first_valid_im, cax=cbar_ax)\n",
    "                cbar.set_label('Jensen-Shannon Distance', fontsize=14)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.subplots_adjust(top=0.9, bottom=0.1, right=0.9)\n",
    "            \n",
    "            # Save best_jsd directly in step folder, others in subfolders\n",
    "            if variant == 'best_jsd':\n",
    "                plt.savefig(os.path.join(step5_dir, f'step5_MedInfF_{MedInfF_val:.2f}_{variant}.png'), \n",
    "                           dpi=300, bbox_inches='tight')\n",
    "            else:\n",
    "                variant_dir = os.path.join(step5_dir, variant)\n",
    "                os.makedirs(variant_dir, exist_ok=True)\n",
    "                plt.savefig(os.path.join(variant_dir, f'step5_MedInfF_{MedInfF_val:.2f}_{variant}.png'), \n",
    "                           dpi=300, bbox_inches='tight')\n",
    "            \n",
    "            plt.close()\n",
    "\n",
    "print(\"All color plots generated successfully!\")\n",
    "print(f\"Plots saved to: {plot_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5274dc4f-55b5-49f7-b740-714bd8e4d147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test block for Step 5 with categorical treatment of parameters\n",
    "def create_categorical_heatmap(pivot_table, xlabel, ylabel, title, norm, cmap='viridis_r', ax=None):\n",
    "    \"\"\"Create a categorical heatmap with proper discrete parameter handling\"\"\"\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Get the unique parameter values\n",
    "    x_categories = pivot_table.columns.tolist()\n",
    "    y_categories = pivot_table.index.tolist()\n",
    "\n",
    "    # Add grid lines between cells\n",
    "    ax.set_xticks(np.arange(len(x_categories)+1)-0.5, minor=True)\n",
    "    ax.set_yticks(np.arange(len(y_categories)+1)-0.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"white\", linestyle='-', linewidth=1)\n",
    "    ax.tick_params(which=\"minor\", size=0)\n",
    "\n",
    "    \n",
    "    # Create the heatmap\n",
    "    im = ax.imshow(pivot_table.values, origin='lower', aspect='auto',\n",
    "                   norm=norm, cmap=cmap)\n",
    "    \n",
    "    # Set ticks and labels\n",
    "    ax.set_xticks(np.arange(len(x_categories)))\n",
    "    ax.set_yticks(np.arange(len(y_categories)))\n",
    "    ax.set_xticklabels([f\"{x:.2f}\" for x in x_categories])\n",
    "    ax.set_yticklabels([f\"{y:.2f}\" for y in y_categories])\n",
    "    \n",
    "    ax.set_xlabel(xlabel, fontsize=14)\n",
    "    ax.set_ylabel(ylabel, fontsize=14)\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    \n",
    "    return im\n",
    "\n",
    "# Test with a specific variant and MedInfF value from Step 5\n",
    "test_variant = 'best_jsd'\n",
    "test_MedInfF = step5_data['MedInfF'].iloc[0]  # Use the first value for testing\n",
    "\n",
    "# Get unique parameter values for the entire step\n",
    "unique_epsM = sorted(step5_data['epsM'].unique())\n",
    "unique_epsSD = sorted(step5_data['epsSD'].unique())\n",
    "unique_MedM = sorted(step5_data['MedM'].unique())\n",
    "unique_MedSD = sorted(step5_data['MedSD'].unique())\n",
    "\n",
    "# Filter data for the test MedInfF value\n",
    "test_data = step5_data[step5_data['MedInfF'] == test_MedInfF]\n",
    "\n",
    "# Create subplot grid\n",
    "fig, axes = plt.subplots(len(unique_MedSD), len(unique_MedM), \n",
    "                        figsize=(6*len(unique_MedM), 5*len(unique_MedSD)),\n",
    "                        sharex=True, sharey=True)\n",
    "\n",
    "if len(unique_MedSD) == 1 and len(unique_MedM) == 1:\n",
    "    axes = np.array([[axes]])\n",
    "elif len(unique_MedSD) == 1:\n",
    "    axes = axes[np.newaxis, :]\n",
    "elif len(unique_MedM) == 1:\n",
    "    axes = axes[:, np.newaxis]\n",
    "\n",
    "# Get global norm for this variant\n",
    "norm = global_norms[test_variant]\n",
    "\n",
    "# Find the first valid subplot to create colorbar\n",
    "first_valid_im = None\n",
    "\n",
    "for i, MedSD_val in enumerate(unique_MedSD):\n",
    "    for j, MedM_val in enumerate(unique_MedM):\n",
    "        ax = axes[i, j]\n",
    "        \n",
    "        # Filter data for this parameter combination\n",
    "        param_data = test_data[\n",
    "            (test_data['MedSD'] == MedSD_val) & \n",
    "            (test_data['MedM'] == MedM_val)\n",
    "        ]\n",
    "        \n",
    "        if len(param_data) > 0:\n",
    "            variant_data = process_variant_data(param_data, test_variant)\n",
    "            if len(variant_data) > 0:\n",
    "                # Create pivot table and ensure all parameter combinations are represented\n",
    "                pivot = variant_data.pivot_table(index='epsSD', columns='epsM', values='distance_final')\n",
    "                \n",
    "                # Reindex to include all possible parameter values\n",
    "                pivot = pivot.reindex(index=unique_epsSD, columns=unique_epsM)\n",
    "                \n",
    "                im = create_categorical_heatmap(pivot, '', '', '', norm, ax=ax)\n",
    "                if first_valid_im is None:\n",
    "                    first_valid_im = im\n",
    "                \n",
    "                # Add parameter labels to subplots\n",
    "                if i == len(unique_MedSD) - 1:\n",
    "                    ax.set_xlabel(param_labels['epsM'], fontsize=12)\n",
    "                if j == 0:\n",
    "                    ax.set_ylabel(param_labels['epsSD'], fontsize=12)\n",
    "                \n",
    "                # Add MedM and MedSD values to subplot title\n",
    "                ax.set_title(f'{param_labels[\"MedM\"]}={MedM_val:.2f}\\n{param_labels[\"MedSD\"]}={MedSD_val:.2f}', \n",
    "                           fontsize=11)\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle(f'Step 5: {test_variant.replace(\"_\", \" \").title()}\\n'\n",
    "            f'{param_labels[\"MedInfF\"]} = {test_MedInfF:.2f}', \n",
    "            fontsize=18, y=0.95)\n",
    "\n",
    "# Add single colorbar for the entire figure\n",
    "if first_valid_im is not None:\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "    cbar = fig.colorbar(first_valid_im, cax=cbar_ax)\n",
    "    cbar.set_label('Jensen-Shannon Distance', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9, bottom=0.1, right=0.9)\n",
    "\n",
    "# Save the test plot\n",
    "test_dir = os.path.join(plot_output_dir, \"test\")\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "plt.savefig(os.path.join(test_dir, f'step5_test_{test_variant}_MedInfF_{test_MedInfF:.2f}.png'), \n",
    "           dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"Test plot created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f040af-77bb-4865-b302-0976ace6e95a",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0ce9e4-9ad9-439f-a020-7a783651198b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
