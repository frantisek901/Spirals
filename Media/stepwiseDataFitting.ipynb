{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b24142ec-d87b-4bca-9d39-4d359840455c",
   "metadata": {},
   "source": [
    "# Stepwise Data Fitting\n",
    "\n",
    "This is an attempt to incrementally analyze the model as we complicate it slowly by adding new dynamics (except for identity which has been left out here). \n",
    "\n",
    "\n",
    "## Models and Simulations\n",
    "Every simulation run has 1000 agents that represent humans, and the richest model has 10 media agents as well as a variable media influence factor. For the first three steps, every parameter combination is run with 30 random initializations. Steps 4 and 5 have been run with 5 random initializations.\n",
    "\n",
    "The following models were tried:\n",
    "\n",
    "1. Simple Hegselmann-Krause with full network.\n",
    "2. Simple Hegselmann-Krause with scale-free network.\n",
    "3. Hegselmann-Krause with normally distributed open-mindedness and scale-free network.\n",
    "3. Hegselmann-Krause with normally distributed open-mindedness, normally distributed initial opinion, and scale-free network.\n",
    "5. Hegselmann-Krause with normally distributed open-mindedness, normally distributed initial opinion, media agents ($MedN = 10$), variable media distribution parameters, variable media influence factor, and scale-free network.\n",
    "6. Hegselmann-Krause with normally distributed open-mindedness, normally distributed initial opinion, media agents ($MedN = 10$), variable media distribution parameters, variable media influence factor, silence mechanisms with memory, and scale-free network.\n",
    "\n",
    "**Notes:** \n",
    "1. The media influence factor  $MedInf \\epsilon [0, 1]$ and is a weighting factor for media-sourced influence. \n",
    "So $MedInf = 0$ implies no influence from media sources, while $MedInf = 1$ implies that media-sourced influence is as strong as other people's opinions.\n",
    "2. The scale-free network parameter is set to 1.\n",
    "3. For Step 4, initial opinion distribution parameters were fixed to values of $\\mu_{InitialPopOp} = 0$ and $\\sigma_{InitialPopOp} = 0.8$ after pilot simulations showed significantly good fits for these values.\n",
    "4. Media opinion positions are drawn from a deterministic set of positions approximating the normal pdf, and have the parameters $\\mu_{MediaOp}$ and $\\sigma_{MediaOp}$.\n",
    "\n",
    "| Model    | Parameters | Random Initializations | Total Simulations |\n",
    "| -------- | ------- | ------- | ------- |\n",
    "| Step 1  | $\\varepsilon$ | 30 | | \n",
    "| Step 2  | $\\varepsilon$ | 30 | | \n",
    "| Step 3  | $\\mu_{\\varepsilon}$, $\\sigma_{\\varepsilon}$| 30 | | \n",
    "| Step 4  | $\\mu_{\\varepsilon}$, $\\sigma_{\\varepsilon}$, $\\sigma_{InitialOp}$| 5 | | \n",
    "| Step 5  | $\\mu_{\\varepsilon}$, $\\sigma_{\\varepsilon}$,  $\\mu_{MediaOp}$, $\\sigma_{MediaOp}$, $MedInf$| 5 | | \n",
    "| Step 6  | $\\mu_{\\varepsilon}$, $\\sigma_{\\varepsilon}$,  $\\mu_{MediaOp}$, $\\sigma_{MediaOp}$, $MedInf$, $Silence_{\\tau}$,  $Silence_{\\delta_{0}}$,  $Silence_{\\alpha}$| 2 | | \n",
    "\n",
    "## Survey Data\n",
    "\n",
    "<>\n",
    "\n",
    "### Parlemeter Data - Initial fitting patterns\n",
    "\n",
    "<>\n",
    "\n",
    "### Dynamic Data - \n",
    "\n",
    "<>\n",
    "\n",
    "## Fitting Strategy\n",
    "\n",
    "<>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b33717f-9d61-4c11-9f7f-c3501840139a",
   "metadata": {},
   "source": [
    "# Preprocessing and Saving Simulation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a77d65-0bcc-466e-bfcc-746b6996fec6",
   "metadata": {},
   "source": [
    "## Globals (SET EACH TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e23037b-9022-45d5-a90f-4427111fb3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_runs_title = \"15.09.25\"\n",
    "\n",
    "# Set below to True if rerunning after saving appropriately in //data//preprocessed//<current_runs_title>\n",
    "load_preprocessed_data = True\n",
    "\n",
    "# Set below to True if there is a need to preprocess data (such as while running for the first time for new data)\n",
    "preprocess_data = False\n",
    "\n",
    "# Set below to True if running for the first time or need to preprocess and save for some other reason\n",
    "save_preprocessed_data = False\n",
    "\n",
    "\n",
    "save_fits = True\n",
    "\n",
    "\n",
    "# set a smaller processing unit if it benefits you, else keep it to range(1, 5)\n",
    "steps_to_process =  range(1, 7)\n",
    "\n",
    "# For the folder names for each step\n",
    "step_input_folder_name = []\n",
    "step_output_folder_name = []\n",
    "\n",
    "# Specific sub-folder names (folder name within data/cluster/endsim)\n",
    "step_input_folder_name.append(\"Step1\") \n",
    "step_input_folder_name.append(\"Step2\") \n",
    "step_input_folder_name.append(\"Step3\") \n",
    "step_input_folder_name.append(\"Step4_5reps\") \n",
    "step_input_folder_name.append(\"Step5_5reps\") \n",
    "step_input_folder_name.append(\"Step6_2reps\") \n",
    "\n",
    "step_output_folder_name = step_input_folder_name\n",
    "\n",
    "step_titles = [\"Simple HK with Full Network\", \"Simple HK with Scale-Free Network\", \"Heterogenous Openness\", \"Heterogenous Openness with normal opinion distributions\", \"Media and Influence\", \"Media with Silence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cfaabfd-0b4d-4b34-b958-9afea2878611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import jensenshannon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e5b323-a936-4157-81a2-acb1cada7f9a",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b39b9808-b30f-4db0-a713-2e1adfa166f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the preprocessed data will be saved\n",
    "full_preprocessed_data_folder_path = os.path.join(\"data\\\\preprocessed\\\\\" + current_runs_title)\n",
    "\n",
    "\n",
    "# Wheer the output plots will be saved\n",
    "plots_folder_path =  os.path.join(\"analysis\\\\plots\\\\\" + current_runs_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc69555-be5b-47dd-b210-bd30b92515c6",
   "metadata": {},
   "source": [
    "## Issue - Step 4 has a LOT OF redundancies and no media\n",
    "\n",
    "I accidentally ran this step with 0 media agents, so we have a great number of repeat simulations and no media in this step.\n",
    "We will rectify this by adding a step 5 with media and rerunning the simulations with the new setting.\n",
    "\n",
    "However, for the time being we need to remove the extra simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83b39c82-ad13-411a-9548-16aa782f71d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Define the regex pattern to extract parameters\n",
    "# pattern = r\"EndSim_epsM(-?[\\d\\.]+)_epsSD(-?[\\d\\.]+)___OpD([a-zA-Z-]+)_OpM(-?[\\d\\.]+)_OpSD(-?[\\d\\.]+)___Net([a-zA-Z-]+)___NAgents(\\d+)___RS(\\d+)__MedInfF([\\d\\.]+)___MedD([a-zA-Z-]+)_MedN(\\d+)_MedM(-?[\\d\\.]+)_MedSD(-?[\\d\\.]+)\"\n",
    "\n",
    "# def extract_eps_params(filename):\n",
    "#     \"\"\"Extract epsM and epsSD parameters from filename\"\"\"\n",
    "#     base_name = os.path.splitext(filename)[0]\n",
    "#     match = re.search(pattern, base_name)\n",
    "#     if match:\n",
    "#         epsM = float(match.group(1))\n",
    "#         epsSD = float(match.group(2))\n",
    "#         RS = int(match.group(8))\n",
    "#         return (epsM, epsSD, RS)\n",
    "#     return None\n",
    "    \n",
    "# # Alternative: Move files instead of deleting (safer approach)\n",
    "# def move_redundant_files(directory, backup_dir=\"redundant_backup\"):\n",
    "#     \"\"\"Move redundant files to a backup directory instead of deleting\"\"\"\n",
    "#     # Create backup directory if it doesn't exist\n",
    "#     backup_path = os.path.join(directory, backup_dir)\n",
    "#     os.makedirs(backup_path, exist_ok=True)\n",
    "    \n",
    "#     # Get all CSV files\n",
    "#     csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "    \n",
    "#     unique_combinations = {}\n",
    "#     files_to_keep = []\n",
    "#     files_to_move = []\n",
    "    \n",
    "#     # Process each file\n",
    "#     for filename in csv_files:\n",
    "#         params = extract_eps_params(filename)\n",
    "#         if params:\n",
    "#             epsM, epsSD, RS = params\n",
    "#             combination = (epsM, epsSD, RS)\n",
    "            \n",
    "#             if combination not in unique_combinations:\n",
    "#                 unique_combinations[combination] = filename\n",
    "#                 files_to_keep.append(filename)\n",
    "#             else:\n",
    "#                 files_to_move.append(filename)\n",
    "#         else:\n",
    "#             print(f\"Warning: Could not parse parameters from {filename}\")\n",
    "    \n",
    "#     # Move redundant files\n",
    "#     moved_count = 0\n",
    "#     for filename in files_to_move:\n",
    "#         src_path = os.path.join(directory, filename)\n",
    "#         dest_path = os.path.join(backup_path, filename)\n",
    "#         try:\n",
    "#             os.rename(src_path, dest_path)\n",
    "#             moved_count += 1\n",
    "#             # print(f\"Moved to backup: {filename}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error moving {filename}: {e}\")\n",
    "    \n",
    "#     # Print summary\n",
    "#     print(f\"\\nSummary:\")\n",
    "#     print(f\"Total files processed: {len(csv_files)}\")\n",
    "#     print(f\"Unique (epsM, epsSD, RS) combinations: {len(unique_combinations)}\")\n",
    "#     print(f\"Files kept: {len(files_to_keep)}\")\n",
    "#     print(f\"Files moved to backup: {moved_count}\")\n",
    "    \n",
    "#     return files_to_keep, files_to_move\n",
    "\n",
    "\n",
    "# move_redundant_files(\"data/cluster/endsim/step4_5reps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5ab314-f43b-4283-8500-b4bb1fa046e9",
   "metadata": {},
   "source": [
    "##  Reading and Preprocessing Simulation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b9ae1d2-7810-496c-a9d6-1cf9be0a248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex pattern for all files\n",
    "pattern = (\n",
    "    r\"EndSim_epsM(-?[\\d\\.]+)_epsSD(-?[\\d\\.]+)\"\n",
    "    r\"___OpD([a-zA-Z-]+)_OpM(-?[\\d\\.]+)_OpSD(-?[\\d\\.]+)\"\n",
    "    r\"___Net([a-zA-Z-]+)___NAgents(\\d+)___RS(\\d+)\"\n",
    "    r\"__MedInfF([\\d\\.]+)___MedD([a-zA-Z-]+)_MedN(\\d+)_MedM(-?[\\d\\.]+)_MedSD(-?[\\d\\.]+)\"\n",
    "    r\"(_SA(-?[\\d\\.]+)_ST(-?[\\d\\.]+)_SDO(-?[\\d\\.]+)_SBB([a-zA-Z]+))?\"\n",
    ")\n",
    "\n",
    "def parse_filename(filename):\n",
    "    \"\"\"Extract parameters from filename using regex with optional silence parameters\"\"\"\n",
    "    # Remove .csv extension before parsing\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "    match = re.search(pattern, base_name)\n",
    "    \n",
    "    if match:\n",
    "        result = {\n",
    "            # 'filename': filename,\n",
    "            'epsM': float(match.group(1)),\n",
    "            'epsSD': float(match.group(2)),\n",
    "            'OpD': match.group(3),\n",
    "            'OpM': float(match.group(4)),\n",
    "            'OpSD': float(match.group(5)),\n",
    "            'NetworkType': match.group(6),\n",
    "            'NAgents': int(match.group(7)),\n",
    "            'RandomSeed': int(match.group(8)),\n",
    "            'MedInfF': float(match.group(9)),\n",
    "            'MedD': match.group(10),\n",
    "            'MedN': int(match.group(11)),\n",
    "            'MedM': float(match.group(12)),\n",
    "            'MedSD': float(match.group(13))\n",
    "        }\n",
    "        # Check if silence parameters are present\n",
    "        if match.group(14):  # The entire silence block exists\n",
    "            # silence_parsed_counts = silence_parsed_counts + 1 # increment the silence parsed counter\n",
    "            result.update({\n",
    "                'Silence_Alpha': float(match.group(15)),\n",
    "                'Silence_Tau': float(match.group(16)),\n",
    "                'Silence_Delta0': float(match.group(17)),\n",
    "                'SilenceByBoundary': match.group(18).lower() == 'true'\n",
    "            })\n",
    "        else:\n",
    "            # no_silence_parsed_counts = no_silence_parsed_counts + 1\n",
    "            # Set default values for silence parameters if not present\n",
    "            result.update({\n",
    "                'Silence_Alpha': None,\n",
    "                'Silence_Tau': None,\n",
    "                'Silence_Delta0': None,\n",
    "                'SilenceByBoundary': None\n",
    "            })\n",
    "        \n",
    "        return result\n",
    "    return None\n",
    "    \n",
    "def extract_opinion_sections(content):\n",
    "    \"\"\"Extract media and individual opinion sections from file content\"\"\"\n",
    "    # Split content into sections using separator lines\n",
    "    sections = re.split(r'-{10,}', content)\n",
    "    \n",
    "    # We expect at least 4 sections:\n",
    "    # 0: Metadata\n",
    "    # 1: Network connections (skip)\n",
    "    # 2: Media opinions\n",
    "    # 3: Individual opinions\n",
    "    if len(sections) < 4:\n",
    "        return None, None\n",
    "    \n",
    "    # Extract media opinions section\n",
    "    media_section = sections[2].strip()\n",
    "    # Extract individual opinions section\n",
    "    individual_section = sections[3].strip()\n",
    "    \n",
    "    return media_section, individual_section\n",
    "\n",
    "def parse_opinion_section(section):\n",
    "    \"\"\"Parse a single opinion section into a DataFrame\"\"\"\n",
    "    if not section:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Clean section by removing any empty lines and brackets\n",
    "    cleaned_lines = []\n",
    "    for line in section.split('\\n'):\n",
    "        if line.strip() and not line.startswith('[') and not line.startswith('--'):\n",
    "            cleaned_lines.append(line)\n",
    "    \n",
    "    if not cleaned_lines:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Recreate CSV content\n",
    "    csv_content = \"\\n\".join(cleaned_lines)\n",
    "    \n",
    "    # Parse as CSV\n",
    "    try:\n",
    "        return pd.read_csv(StringIO(csv_content))\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def process_simulation_file(filepath):\n",
    "    \"\"\"Process a single simulation file with section handling\"\"\"\n",
    "    # Extract parameters from filename\n",
    "    filename = os.path.basename(filepath)\n",
    "    params = parse_filename(filename)\n",
    "    if not params:\n",
    "        print(f\"Skipping {filename} - pattern not matched\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Read entire file content\n",
    "        with open(filepath, 'r') as f:\n",
    "            content = f.read()\n",
    "        # Extract opinion sections\n",
    "        media_section, individual_section = extract_opinion_sections(content)\n",
    "\n",
    "        # Parse media opinions\n",
    "        media_df = parse_opinion_section(media_section)\n",
    "\n",
    "        media_opinions = []\n",
    "        # print(media_df.columns)\n",
    "\n",
    "        if not media_df.empty and ' opinion)' in media_df:\n",
    "            media_opinions = media_df[' opinion)'].apply(\n",
    "                lambda x: float(str(x).strip('[]'))).values\n",
    "        \n",
    "        # Parse individual opinions\n",
    "        individual_df = parse_opinion_section(individual_section)\n",
    "        individual_opinions = []\n",
    "        if not individual_df.empty and 'opinion' in individual_df:\n",
    "            individual_opinions = individual_df['opinion'].apply(\n",
    "                lambda x: float(str(x).strip('[]'))).values\n",
    "        \n",
    "        \n",
    "        individual_initial_opinions = []\n",
    "        if not individual_df.empty and 'initialOpinion' in individual_df:\n",
    "            individual_initial_opinions = individual_df['initialOpinion'].apply(\n",
    "                lambda x: float(str(x).strip('[]'))).values\n",
    "        \n",
    "        # Add to params\n",
    "        params['individual_opinions'] = np.array(individual_opinions)\n",
    "        params['individual_initial_opinions'] = np.array(individual_initial_opinions)\n",
    "        params['media_opinions'] = np.array(media_opinions)\n",
    "        \n",
    "        # Add static parameters\n",
    "        params['NAgents'] = 1000\n",
    "        params['MedN'] = 20\n",
    "        params['NetType'] = \"scale-free\"\n",
    "        params['ScaleFreeDegree'] = 1\n",
    "        \n",
    "        return params\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_all_simulations(directory):\n",
    "    \"\"\"Process all simulation files in a directory\"\"\"\n",
    "    all_data = []\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.csv'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            result = process_simulation_file(filepath)\n",
    "            if result:\n",
    "                all_data.append(result)\n",
    "                processed_count += 1\n",
    "            else:\n",
    "                skipped_count += 1\n",
    "            if(processed_count%1000 == 0):\n",
    "                print(f\"Processed {processed_count} sims for next step.\")\n",
    "    print(f\"\\n\\nSummary: Processed={processed_count}, Skipped={skipped_count}\")\n",
    "    return pd.DataFrame(all_data) if all_data else pd.DataFrame()\n",
    "\n",
    "def get_simulations_for_step(stepNo):\n",
    "    simulations_df = load_all_simulations(\"data\\\\cluster\\\\endsim\\\\\" + step_input_folder_name[stepNo - 1])\n",
    "    \n",
    "    if not simulations_df.empty:\n",
    "        print(f\"\\nSuccessfully loaded {len(simulations_df)} simulations for Step {stepNo}\")\n",
    "        print(\"Parameter ranges:\")\n",
    "        for param in ['epsM', 'epsSD', 'OpM', 'OpSD', 'MedM', 'MedSD', 'MedInfF']:\n",
    "            values = simulations_df[param]\n",
    "            print(f\"{param}: min={values.min()}, max={values.max()}\")\n",
    "        \n",
    "        # Show example opinion data\n",
    "        sample = simulations_df.iloc[0]\n",
    "        print(f\"\\nSample simulation opinions:\")\n",
    "        print(f\"Individual opinions: {len(sample['individual_opinions'])} values\")\n",
    "        print(f\"Media opinions: {len(sample['media_opinions'])} values\")\n",
    "    else:\n",
    "        print(f\"No simulations loaded for Step {stepNo}. Please check file patterns and data quality.\")\n",
    "\n",
    "    return(simulations_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc800754-60a5-4c4f-bad6-ec947166e056",
   "metadata": {},
   "source": [
    "## Loading up Simulation Data into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bee8fa58-0987-4d79-9235-49c605ed2356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Load simulation data\n",
    "simulation_df_step = []    # The main list of Dataframes in which we will be loading the data\n",
    "\n",
    "if(preprocess_data):\n",
    "    # Running all sims and storing them as DF's\n",
    "    for stepNo in steps_to_process:\n",
    "        simulation_df_step.append(get_simulations_for_step(stepNo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ce032a2-a1c0-4227-aa26-ead1f9bea47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulation_df_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dce8da1-c172-454c-b90b-b2dc0940a53f",
   "metadata": {},
   "source": [
    "## Saving Preprocessed Simulation Dataframes as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ace620a-6e0d-4e53-8f45-0c00b732a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(save_preprocessed_data and preprocess_data):\n",
    "    if not os.path.exists(full_preprocessed_data_folder_path):\n",
    "        os.makedirs(full_preprocessed_data_folder_path)\n",
    "    for stepNo in steps_to_process:\n",
    "        file_name = f\"simulated_data_Step{stepNo}.csv\"\n",
    "        file_path = os.path.join(full_preprocessed_data_folder_path, file_name)\n",
    "        simulation_df_step[stepNo - 1].to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4d8b51-2b8c-4ea0-988c-69d9bf6e03fd",
   "metadata": {},
   "source": [
    "## Loading Preprocessed Simulation Dataframes from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd2080b-b6e7-4017-b309-cd868a80ca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(load_preprocessed_data):\n",
    "    del simulation_df_step\n",
    "    simulation_df_step = [] # Don't do this unless we are loading subsequently, otherwise we will have an emptied list and much sadness to go with.\n",
    "    for stepNo in steps_to_process:\n",
    "        file_name = f\"simulated_data_Step{stepNo}.csv\"\n",
    "        file_path = os.path.join(full_preprocessed_data_folder_path, file_name)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Need to convert the arrays to numpy arrays\n",
    "        individual_opinions = []\n",
    "        if not df.empty and 'individual_opinions' in df:\n",
    "            df['individual_opinions'] = df['individual_opinions'].apply(\n",
    "                lambda x: np.fromstring(x.strip('[]').replace('\\n', ' '), sep=' ', dtype=float))\n",
    "        \n",
    "        individual_initial_opinions = []\n",
    "        if not df.empty and 'individual_initial_opinions' in df:\n",
    "            df['individual_initial_opinions'] = df['individual_initial_opinions'].apply(\n",
    "                lambda x: np.fromstring(x.strip('[]').replace('\\n', ' '), sep=' ', dtype=float))\n",
    "        \n",
    "        simulation_df_step.append(df) # append to the stepwise dataframe list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04a2b91-c8f6-4040-b909-6f39ba90c54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_df_step[3]['media_opinions']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57ec621-4725-40b1-b5c3-b2a8cba06279",
   "metadata": {},
   "source": [
    "## For Binning Simulation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a41a7e-4f12-409d-a30e-4380d0347055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bin edges for 10-point scale mapping\n",
    "bin_edges = np.array([-1.0, -0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "\n",
    "# Create bin interval labels for display\n",
    "bin_intervals = [\n",
    "    \"[-1.0, -0.8)\",\n",
    "    \"[-0.8, -0.6)\",\n",
    "    \"[-0.6, -0.4)\",\n",
    "    \"[-0.4, -0.2)\",\n",
    "    \"[-0.2, 0.0)\",\n",
    "    \"[0.0, 0.2)\",\n",
    "    \"[0.2, 0.4)\",\n",
    "    \"[0.4, 0.6)\",\n",
    "    \"[0.6, 0.8)\",\n",
    "    \"[0.8, 1.0]\"\n",
    "]\n",
    "\n",
    "def bin_opinions(opinions):\n",
    "    \"\"\"Bin opinions into 10-point scale categories\"\"\"\n",
    "    # Digitize opinions (returns bin indices 1-10)\n",
    "    bin_indices = np.digitize(opinions, bin_edges[1:-1], right=False)\n",
    "    return bin_indices\n",
    "\n",
    "def bin_distribution(opinions):\n",
    "    \"\"\"Compute normalized distribution across bins\"\"\"\n",
    "    counts, _ = np.histogram(opinions, bins=bin_edges)\n",
    "    return counts / counts.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c21053-1ae5-4cca-b9bb-ef393c80faaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding binned columns to DF\n",
    "\n",
    "for stepNo in steps_to_process:\n",
    "    simulations_df = simulation_df_step[stepNo - 1]\n",
    "    \n",
    "    # Add binned opinions and distributions to DataFrame\n",
    "    simulations_df['binned_opinions'] = simulations_df['individual_opinions'].apply(\n",
    "        lambda x: bin_opinions(np.array(x))\n",
    "    )\n",
    "    \n",
    "    simulations_df['binned_distribution'] = simulations_df['individual_opinions'].apply(\n",
    "        lambda x: bin_distribution(np.array(x))\n",
    "    )\n",
    "    \n",
    "    # Add bin interval labels as a constant column\n",
    "    simulations_df['bin_intervals'] = [bin_intervals] * len(simulations_df)\n",
    "\n",
    "    # Club the two central bins to account for anchoring in the real data\n",
    "    simulations_df['binned_distributions_5_6clubbed'] = simulations_df['binned_distribution'].apply(\n",
    "        lambda dist: np.concatenate([dist[:4], [dist[4] + dist[5]], dist[6:]])\n",
    "    )\n",
    "\n",
    "    # Repeating the same for the initial opinion distribution\n",
    "\n",
    "    # Add binned opinions and distributions to DataFrame\n",
    "    simulations_df['binned_initial_opinions'] = simulations_df['individual_initial_opinions'].apply(\n",
    "        lambda x: bin_opinions(np.array(x))\n",
    "    )\n",
    "    \n",
    "    simulations_df['binned_initial_distribution'] = simulations_df['individual_initial_opinions'].apply(\n",
    "        lambda x: bin_distribution(np.array(x))\n",
    "    )\n",
    "\n",
    "    # Club the two central bins to account for anchoring in the real data\n",
    "    simulations_df['binned_initial_distributions_5_6clubbed'] = simulations_df['binned_initial_distribution'].apply(\n",
    "        lambda dist: np.concatenate([dist[:4], [dist[4] + dist[5]], dist[6:]])\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "    # Display the updated DataFrame\n",
    "    # print(\"\\nDataFrame with binned opinions:\")\n",
    "    # print(simulations_df[['epsM', 'epsSD', 'OpM', 'OpSD', 'MedM', 'MedSD', \n",
    "    #                       'binned_opinions', 'binned_distribution', 'bin_intervals']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab151fd-3462-4dd9-bbab-71228675a0da",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61328aa6-9bc9-4ab7-afd6-f91285b83599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a Simulation Index column to the dataframes\n",
    "\n",
    "\n",
    "# Initialize cumulative index counter\n",
    "cumulative_index = 0\n",
    "\n",
    "\n",
    "for stepNo in steps_to_process:\n",
    "    # Get the current DataFrame\n",
    "    simulations_df = simulation_df_step[stepNo - 1]\n",
    "    \n",
    "    # Add model simulation index (starts at 1 for each DataFrame)\n",
    "    simulations_df['model_simulation_index'] = range(1, len(simulations_df) + 1)\n",
    "    \n",
    "    # Add cumulative simulation index (continues from previous DataFrame)\n",
    "    simulations_df['cumulative_simulation_index'] = range(cumulative_index + 1, \n",
    "                                                         cumulative_index + len(simulations_df) + 1)\n",
    "    \n",
    "    # Update cumulative index for next DataFrame\n",
    "    cumulative_index += len(simulations_df)\n",
    "    \n",
    "    # Update the DataFrame in the list\n",
    "    simulation_df_step[stepNo - 1] = simulations_df    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64872d6-76ac-4a54-9a87-5bfb7b1bb1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for stepNo in steps_to_process:\n",
    "    simulations_df = simulation_df_step[stepNo - 1]\n",
    "    print(f\"Step {stepNo}:\")\n",
    "    print(f\"  Model indices: {simulations_df['model_simulation_index'].min()} to {simulations_df['model_simulation_index'].max()}\")\n",
    "    print(f\"  Cumulative indices: {simulations_df['cumulative_simulation_index'].min()} to {simulations_df['cumulative_simulation_index'].max()}\")\n",
    "    print(f\"  Number of simulations: {len(simulations_df)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7ffb16-9592-4221-8d57-8a8257dfecfc",
   "metadata": {},
   "source": [
    "## Pooled Histograms for each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d34be07-9e2c-4176-ab9a-98bc6bee48f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stepNo in steps_to_process:\n",
    "    simulations_df = simulation_df_step[stepNo - 1]\n",
    "    # Create pooled histogram of all opinions\n",
    "    all_opinions = np.concatenate(simulations_df['individual_opinions'].values)\n",
    "    pooled_counts, _ = np.histogram(all_opinions, bins=bin_edges)\n",
    "    pooled_distribution = pooled_counts / pooled_counts.sum()\n",
    "    \n",
    "    # Plot the pooled histogram\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(1, 11), pooled_distribution, color='skyblue', edgecolor='black')\n",
    "    plt.xticks(range(1, 11), bin_intervals, rotation=45, ha='right')\n",
    "    plt.xlabel('Opinion Bins')\n",
    "    plt.ylabel('Proportion of Opinions')\n",
    "    plt.title('Pooled Opinion Distribution Across All Simulations in ' +  step_titles[stepNo - 1] + \" (Step \" +  str(stepNo)+ \")\")\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pooled_opinion_distribution.png', dpi=300)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef2fd40-1483-4816-931d-2fb91f8c2cd0",
   "metadata": {},
   "source": [
    "## Generating Individual histograms for specific simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eccc6a-5a51-45a5-93aa-d6194efc0348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Define bin edges and labels\n",
    "bin_edges = np.array([-1.0, -0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "bin_intervals = [\n",
    "    \"[-1.0, -0.8)\", \"[-0.8, -0.6)\", \"[-0.6, -0.4)\", \"[-0.4, -0.2)\", \n",
    "    \"[-0.2, 0.0)\", \"[0.0, 0.2)\", \"[0.2, 0.4)\", \"[0.4, 0.6)\", \n",
    "    \"[0.6, 0.8)\", \"[0.8, 1.0]\"\n",
    "]\n",
    "\n",
    "def save_simulation_histogram(row, index, stepNum, histograms_save_path):\n",
    "    # histograms_save_path = os.join(histograms_save_path, step_output_folder_name[stepNum - 1])\n",
    "    \"\"\"Create and save histogram for a single simulation with both initial and final opinions\"\"\"\n",
    "    # Extract parameters for title and filename based on step number\n",
    "    if stepNum == 1 or stepNum == 2:\n",
    "        # Only epsM for steps 1 and 2\n",
    "        filename = f\"sim_{index:04d}_epsM{row['epsM']:.2f}.png\".replace(\".\", \"dot\")\n",
    "        title = f\"Opinion Distribution (Step {stepNum}): εμ={row['epsM']:.2f}\"\n",
    "    elif stepNum == 3:\n",
    "        # epsM and epsSD for step 3\n",
    "        filename = f\"sim_{index:04d}_epsM{row['epsM']:.2f}_epsSD{row['epsSD']:.2f}.png\".replace(\".\", \"dot\")\n",
    "        title = f\"Opinion Distribution (Step {stepNum}): εμ={row['epsM']:.2f}, εσ={row['epsSD']:.2f}\"\n",
    "    else:\n",
    "        # All parameters for step 4 and beyond\n",
    "        filename = (\n",
    "            f\"sim_{index:04d}_\"\n",
    "            f\"epsM{row['epsM']:.2f}_epsSD{row['epsSD']:.2f}_\"\n",
    "            f\"OpM{row['OpM']:.2f}_OpSD{row['OpSD']:.2f}_\"\n",
    "            f\"MedM{row['MedM']:.2f}_MedSD{row['MedSD']:.2f}__\"\n",
    "            f\"MedInf{row['MedInfF']:.2f}.png\"\n",
    "        ).replace(\".\", \"dot\")\n",
    "        title = (\n",
    "            f\"Opinion Distribution (Step {stepNum}): \"\n",
    "            f\"εμ={row['epsM']:.2f}, εσ={row['epsSD']:.2f}, \"\n",
    "            f\"Medμ={row['MedM']:.2f}, Medσ={row['MedSD']:.2f}, \"\n",
    "            f\"MedInfF={row['MedInfF']:.2f}\"\n",
    "        )\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    # Plot histograms for both initial and final opinions\n",
    "    n_initial, _, _ = plt.hist(\n",
    "        row['individual_initial_opinions'],\n",
    "        bins=bin_edges,\n",
    "        density=False,\n",
    "        alpha=0.5,\n",
    "        color='steelblue',\n",
    "        edgecolor='black',\n",
    "        label='Initial Opinions'\n",
    "    )\n",
    "    \n",
    "    n_final, bins, patches = plt.hist(\n",
    "        row['individual_opinions'], \n",
    "        bins=bin_edges, \n",
    "        density=False, \n",
    "        alpha=0.7, \n",
    "        color='green',\n",
    "        edgecolor='black',\n",
    "        label='Final Opinions'\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "    # Add vertical lines at media positions for reference\n",
    "    if(stepNo == 5):\n",
    "        for medop in row['media_opinions']:\n",
    "                plt.axvline(medop, color='red', linestyle='--', alpha=0.5, label='Media Opinion' if medop == row['media_opinions'][0] else \"\")\n",
    "    \n",
    "    # Format plot\n",
    "    plt.title(title, fontsize=14, pad=20)\n",
    "    plt.xlabel('Opinion Value', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.xticks(bin_edges, rotation=45)\n",
    "    plt.xlim(-1.05, 1.05)\n",
    "    # plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Add bin labels\n",
    "    bin_centers = 0.5 * (bin_edges[1:] + bin_edges[:-1])\n",
    "    # for i in range(len(bin_intervals)):\n",
    "    #     plt.text(\n",
    "    #         bin_centers[i], max(n_final[i], n_initial[i]) + 0.01, \n",
    "    #         f\"F:{n_final[i]}\\nI:{n_initial[i]}\\n{bin_intervals[i]}\", \n",
    "    #         ha='center', va='bottom', fontsize=8\n",
    "    #     )\n",
    "    \n",
    "    # Add statistics box\n",
    "    stats_text = (\n",
    "        f\"N: {len(row['individual_opinions'])}\\n\"\n",
    "        f\"Initial Mean: {np.mean(row['individual_initial_opinions']):.3f}\\n\"\n",
    "        f\"Final Mean: {np.mean(row['individual_opinions']):.3f}\\n\"\n",
    "        f\"Change: {np.mean(row['individual_opinions']) - np.mean(row['individual_initial_opinions']):.3f}\"\n",
    "    )\n",
    "    \n",
    "    plt.gcf().text(0.50, 0.85, stats_text, fontsize=10, \n",
    "                   bbox=dict(facecolor='white', alpha=0.5))\n",
    "    \n",
    "    # Save and close\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(histograms_save_path, filename), dpi=150)\n",
    "    plt.close()\n",
    "    return filename\n",
    "\n",
    "# Function to generate histogram by cumulative index\n",
    "def generate_histogram_by_cumulative_index(cumulative_index, histograms_save_path):\n",
    "    \"\"\"Generate histogram for a simulation based on its cumulative index\"\"\"\n",
    "    for stepNo in steps_to_process:\n",
    "        simulations_df = simulation_df_step[stepNo - 1]\n",
    "        if cumulative_index in simulations_df['cumulative_simulation_index'].values:\n",
    "            row = simulations_df[simulations_df['cumulative_simulation_index'] == cumulative_index].iloc[0]\n",
    "            model_index = row['model_simulation_index']\n",
    "            return save_simulation_histogram(row, cumulative_index, stepNo, histograms_save_path)\n",
    "    print(f\"Error: Cumulative index {cumulative_index} not found.\")\n",
    "    return None\n",
    "\n",
    "# Function to generate histogram by step and model index\n",
    "def generate_histogram_by_step_model_index(stepNo, model_index, histograms_save_path):\n",
    "    \"\"\"Generate histogram for a simulation based on step number and model index\"\"\"\n",
    "    if stepNo > len(simulation_df_step):\n",
    "        print(f\"Error: Step number {stepNo} is out of range.\")\n",
    "        return None\n",
    "    \n",
    "    simulations_df = simulation_df_step[stepNo - 1]\n",
    "    if model_index not in simulations_df['model_simulation_index'].values:\n",
    "        print(f\"Error: Model index {model_index} not found in step {stepNo}.\")\n",
    "        return None\n",
    "    \n",
    "    row = simulations_df[simulations_df['model_simulation_index'] == model_index].iloc[0]\n",
    "    cumulative_index = row['cumulative_simulation_index']\n",
    "    return save_simulation_histogram(row, cumulative_index, stepNo, histograms_save_path)\n",
    "\n",
    "# Setup directories for each step\n",
    "for stepNo in steps_to_process:\n",
    "    # Setup Path\n",
    "    histograms_save_path = os.path.join(plots_folder_path, \"selected_sims_histograms\")\n",
    "    if not os.path.exists(histograms_save_path):\n",
    "        os.makedirs(histograms_save_path)\n",
    "    \n",
    "    # You can now call either:\n",
    "    # generate_histogram_by_cumulative_index(1, histograms_save_path)\n",
    "    # or\n",
    "    generate_histogram_by_step_model_index(4, 1, histograms_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda98fab-7ada-4c21-9f77-6925c334cce0",
   "metadata": {},
   "source": [
    "# Load Survey Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597ce113-3675-4090-bafe-90e5e3c91d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "folder_path = \"data/EU_dataset\"\n",
    "file_title = \"Cleaned_Parlemeter_Data-LeftRight\"\n",
    "file_name = file_title + \".xlsx\"\n",
    "survey_file = os.path.join(folder_path, file_name)\n",
    "\n",
    "sheets = pd.ExcelFile(survey_file).sheet_names[1:]  # Skip 'RowHeaders'\n",
    "\n",
    "survey_data = []\n",
    "\n",
    "for sheet in sheets:\n",
    "    country = sheet.split(\"-\")[0]  # Extract country code (e.g., \"BE\")\n",
    "    df_sheet = pd.read_excel(survey_file, sheet_name=sheet, header=None)\n",
    "    \n",
    "    # Years are in the first row\n",
    "    years = df_sheet.iloc[0, :].tolist()\n",
    "    \n",
    "    # Extract counts for positions 1-10 (skip % rows and DK/Refusal)\n",
    "    counts_matrix = []\n",
    "    for i in range(10):  # Positions 1-10\n",
    "        row_index = 1 + 2 * i  # Count rows are at indices 1,3,5,...,19\n",
    "        counts_matrix.append(df_sheet.iloc[row_index, :].tolist())\n",
    "    \n",
    "    # Process each year\n",
    "    for j, year in enumerate(years):\n",
    "        counts = [float(counts_matrix[i][j]) for i in range(10)]\n",
    "        total_valid = sum(counts)\n",
    "        \n",
    "        # Normalized distribution (1-10)\n",
    "        distribution = [c / total_valid for c in counts]\n",
    "\n",
    "        # Pool bins 5 and 6\n",
    "        pooled_counts = counts[:4] + [counts[4] + counts[5]] + counts[6:]\n",
    "        pooled_distribution = [c / total_valid for c in pooled_counts]\n",
    "        \n",
    "        survey_data.append({\n",
    "            \"country\": country,\n",
    "            \"year\": int(year),\n",
    "            \"binned_distribution\": distribution,\n",
    "            \"binned_distribution_5_6clubbed\": pooled_distribution,\n",
    "            \"total_respondents\": total_valid\n",
    "        })\n",
    "survey_df = pd.DataFrame(survey_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75acdfe3-0637-4acf-bd35-87fc5a5523b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simulation_df_step[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5bedba-105d-4e63-9772-7e28a496b195",
   "metadata": {},
   "source": [
    "## Fitting Data for all Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc617a1-5ebc-4598-aec9-93ebd978642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure vectors are numpy arrays of floats\n",
    "def to_array(x):\n",
    "    return np.array(x, dtype=float)\n",
    "\n",
    "# List of dataframes for the fits (now combined initial and final)\n",
    "jsfit_dfs_stepwise = []\n",
    "best_jsfits_stepwise = []\n",
    "\n",
    "for stepNo in steps_to_process:\n",
    "    simulations_df = simulation_df_step[stepNo - 1]\n",
    "    this_fit_data = []\n",
    "    \n",
    "    # Precompute survey distributions once\n",
    "    survey_dists = []\n",
    "    for _, surv_row in survey_df.iterrows():\n",
    "        survey_dists.append({\n",
    "            'country': surv_row['country'],\n",
    "            'year': surv_row['year'],\n",
    "            'distribution': to_array(surv_row['binned_distribution_5_6clubbed'])\n",
    "        })\n",
    "    \n",
    "    for sim_idx, sim_row in simulations_df.iterrows():\n",
    "        sim_dist_final = to_array(sim_row['binned_distributions_5_6clubbed'])\n",
    "        sim_dist_initial = to_array(sim_row['binned_initial_distributions_5_6clubbed'])\n",
    "        # print(sim_row)\n",
    "        # Extract simulation parameters once\n",
    "        sim_params = {\n",
    "            'cum_sim_index': sim_row['cumulative_simulation_index'],\n",
    "            'epsM': sim_row['epsM'],\n",
    "            'epsSD': sim_row['epsSD'],\n",
    "            'OpM': sim_row['OpM'],\n",
    "            'OpSD': sim_row['OpSD'],\n",
    "            'MedM': sim_row['MedM'],\n",
    "            'RS': sim_row['RandomSeed'],\n",
    "            'MedSD': sim_row['MedSD'],\n",
    "            'MedInfF': sim_row['MedInfF']\n",
    "        }\n",
    "        \n",
    "        for surv_data in survey_dists:\n",
    "            surv_dist = surv_data['distribution']\n",
    "            \n",
    "            # Jensen–Shannon divergence for both initial and final\n",
    "            jsd_final = jensenshannon(sim_dist_final, surv_dist)\n",
    "            jsd_initial = jensenshannon(sim_dist_initial, surv_dist)\n",
    "            \n",
    "            # Create a single entry with both distances\n",
    "            this_fit_data.append({\n",
    "                **sim_params,  # Unpack simulation parameters\n",
    "                'country': surv_data['country'],\n",
    "                'year': surv_data['year'],\n",
    "                'distance_final': jsd_final,\n",
    "                'distance_initial': jsd_initial,\n",
    "                'distance_delta': jsd_final - jsd_initial\n",
    "            })\n",
    "    \n",
    "    # Create a single DataFrame for this step\n",
    "    thisfit_df = pd.DataFrame(this_fit_data)\n",
    "    jsfit_dfs_stepwise.append(thisfit_df)\n",
    "    \n",
    "    # Extract best fits for both initial and final\n",
    "    best_fit_final = thisfit_df.sort_values('distance_final').groupby(['country', 'year']).head(5)\n",
    "    best_fit_initial = thisfit_df.sort_values('distance_initial').groupby(['country', 'year']).head(5)\n",
    "    \n",
    "    # Combine best fits into a single DataFrame with a type indicator\n",
    "    best_fit_final['distance_type'] = 'final'\n",
    "    best_fit_initial['distance_type'] = 'initial'\n",
    "    \n",
    "    best_fit_combined = pd.concat([best_fit_final, best_fit_initial], ignore_index=True)\n",
    "    best_jsfits_stepwise.append(best_fit_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed117bf-d57a-4263-b389-6388686f5aa5",
   "metadata": {},
   "source": [
    "## Adding Silence param to step 6 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb136e3-d3a7-4cbc-8933-1b23d0354aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# pick the relevant dfs\n",
    "sim_df = simulation_df_step[5]\n",
    "jsfit_df = jsfit_dfs_stepwise[5]\n",
    "\n",
    "# select only the columns we need from sim_df\n",
    "cols_to_copy = [\n",
    "    \"cumulative_simulation_index\",\n",
    "    \"Silence_Alpha\",\n",
    "    \"Silence_Tau\",\n",
    "    \"Silence_Delta0\",\n",
    "    \"SilenceByBoundary\",\n",
    "]\n",
    "\n",
    "sim_subset = sim_df[cols_to_copy]\n",
    "\n",
    "# merge: match cum_sim_index in jsfit_df to cumulative_simulation_index in sim_subset\n",
    "merged = jsfit_df.merge(\n",
    "    sim_subset,\n",
    "    how=\"left\",  # keep all rows in jsfit_df\n",
    "    left_on=\"cum_sim_index\",\n",
    "    right_on=\"cumulative_simulation_index\",\n",
    ")\n",
    "\n",
    "# drop duplicate key column if you don’t want it twice\n",
    "merged = merged.drop(columns=[\"cumulative_simulation_index\"])\n",
    "\n",
    "# replace back in list if you want\n",
    "jsfit_dfs_stepwise[5] = merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b345550-bec7-475f-bfbf-0447525b77b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jsfit_dfs_stepwise[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c947401b-51e8-4e3c-b7fe-1b457573d2d3",
   "metadata": {},
   "source": [
    "## Saving Fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eedf61-81ff-4680-a161-80a978302e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(save_fits):\n",
    "    if not os.path.exists(full_preprocessed_data_folder_path):\n",
    "        os.makedirs(full_preprocessed_data_folder_path)\n",
    "    for stepNo in steps_to_process:\n",
    "        file_name = f\"JS_fits_Step{stepNo}.csv\"\n",
    "        file_path = os.path.join(full_preprocessed_data_folder_path, file_name)\n",
    "        jsfit_dfs_stepwise[stepNo - 1].to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a332f71-11d4-4a9f-b6e4-10c3e96e6b61",
   "metadata": {},
   "source": [
    "## Plotting JSD v/s Epsilon\n",
    "\n",
    "### Steps 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92c814d-382e-441d-937d-66de58f690ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fit_threshold = 0.05 # Set this to filter the best fits\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "# Create output directory for plots\n",
    "plot_output_dir = os.path.join(plots_folder_path, \"Fit_Plots\")\n",
    "os.makedirs(plot_output_dir, exist_ok=True)\n",
    "\n",
    "# Process steps 1 and 2 (index 0 and 1 in our list)\n",
    "for step_idx in steps_to_process:\n",
    "    stepNo = step_idx + 1  # Convert to 1-based indexing\n",
    "    fit_df = jsfit_dfs_stepwise[step_idx]\n",
    "    \n",
    "    print(f\"Processing Step {stepNo} with {len(fit_df)} data points\")\n",
    "    \n",
    "    # Plot 1: Final distance against epsM\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Group by epsM and calculate mean and standard error\n",
    "    grouped = fit_df.groupby('epsM')['distance_final'].agg(['mean', 'std', 'count'])\n",
    "    grouped['se'] = grouped['std'] / np.sqrt(grouped['count'])\n",
    "\n",
    "    \n",
    "    grouped_initial = fit_df.groupby('epsM')['distance_initial'].agg(['mean', 'std', 'count'])\n",
    "    grouped_initial['se']  =  grouped_initial['std'] / np.sqrt(grouped_initial['count'])\n",
    "\n",
    "    \n",
    "    plt.errorbar(grouped.index, grouped['mean'], yerr=grouped['se'], \n",
    "                 fmt='o-', capsize=5, capthick=2, linewidth=2, markersize=8, label=\"End of Sim divergence\")\n",
    "\n",
    "    #Initial for comparison\n",
    "    plt.errorbar(grouped_initial.index, grouped_initial['mean'], yerr=grouped_initial['se'], \n",
    "                 fmt='o-', capsize=5, capthick=2, linewidth=2, markersize=8, label=\"Beginning of Sim divergence\")\n",
    "    \n",
    "    plt.xlabel('ε (Openness)', fontsize=14)\n",
    "    plt.ylabel('End of Sim Jensen-Shannon Divergence', fontsize=14)\n",
    "    plt.title(f'Final Fit Quality vs Openness (ε)\\n{step_titles[stepNo - 1]}', fontsize=16)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(os.path.join(plot_output_dir, f'step{stepNo}_final_vs_epsM.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 2: Final distance against epsM (only improved fits)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Filter for improved fits (final distance < initial distance)\n",
    "    improved_fits = fit_df[fit_df['distance_delta'] < 0]\n",
    "    \n",
    "    if len(improved_fits) > 0:\n",
    "        # Group by epsM and calculate statistics\n",
    "        grouped_improved = improved_fits.groupby('epsM')['distance_final'].agg(['mean', 'std', 'count'])\n",
    "        grouped_improved['se'] = grouped_improved['std'] / np.sqrt(grouped_improved['count'])\n",
    "        \n",
    "        plt.errorbar(grouped_improved.index, grouped_improved['mean'], yerr=grouped_improved['se'], \n",
    "                     fmt='s-', capsize=5, capthick=2, linewidth=2, markersize=8, color='green', label = \"Improved fits only\")\n",
    "        \n",
    "        # Add original for comparison (light gray)\n",
    "        plt.errorbar(grouped.index, grouped['mean'], yerr=grouped['se'], \n",
    "                     fmt='o--', capsize=3, capthick=1, linewidth=1, markersize=5, \n",
    "                     color='gray', alpha=0.5, label='All simulations')\n",
    "        \n",
    "        plt.xlabel('ε (Openness)', fontsize=14)\n",
    "        plt.ylabel('End of Sim Jensen-Shannon Divergence', fontsize=14)\n",
    "        plt.title(f'Improved Fits Only (Final < Initial)\\n{step_titles[stepNo - 1]}', fontsize=16)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(plot_output_dir, f'step{stepNo}_improved_final_vs_epsM.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(f\"No improved fits found for Step {stepNo}\")\n",
    "        plt.close()\n",
    "    \n",
    "    # Plot 3: Delta distance against epsM\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Group by epsM and calculate mean and standard error for delta\n",
    "    grouped_delta = fit_df.groupby('epsM')['distance_delta'].agg(['mean', 'std', 'count'])\n",
    "    grouped_delta['se'] = grouped_delta['std'] / np.sqrt(grouped_delta['count'])\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.errorbar(grouped_delta.index, grouped_delta['mean'], yerr=grouped_delta['se'], \n",
    "                 fmt='o-', capsize=5, capthick=2, linewidth=2, markersize=8, color='purple')\n",
    "    \n",
    "    # Add horizontal line at zero for reference\n",
    "    plt.axhline(y=0, color='red', linestyle='--', alpha=0.7, linewidth=1)\n",
    "    \n",
    "    plt.xlabel('ε (Openness)', fontsize=14)\n",
    "    plt.ylabel('Δ Divergence (Final - Initial)', fontsize=14)\n",
    "    plt.title(f'Fit Improvement vs ε (Negative values = Improvement)\\n{step_titles[stepNo - 1]}', fontsize=16)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_output_dir, f'step{stepNo}_delta_vs_epsM.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    # Plot 4: Only the best of the best fits.\n",
    "    best_fits = fit_df[fit_df['distance_final'] < best_fit_threshold]\n",
    "    print(\"Best fits dataframe: \\n\")\n",
    "    print(best_fits)\n",
    "    if len(best_fits) > 0:\n",
    "        # Group by epsM and calculate statistics\n",
    "        grouped_best = best_fits.groupby('epsM')['distance_final'].agg(['mean', 'std', 'count'])\n",
    "        grouped_best['se'] = grouped_best['std'] / np.sqrt(grouped_best['count'])\n",
    "\n",
    "        \n",
    "        grouped_best_initial = best_fits.groupby('epsM')['distance_initial'].agg(['mean', 'std', 'count'])\n",
    "        grouped_best_initial['se'] = grouped_best_initial['std'] / np.sqrt(grouped_best_initial['count'])\n",
    "        \n",
    "        plt.errorbar(grouped_best.index, grouped_best['mean'], yerr=grouped_best['se'], \n",
    "                     fmt='s-', capsize=5, capthick=2, linewidth=2, markersize=8, color='green', label = \"End of Sim Divergence\")\n",
    "        \n",
    "        # Initial for comparison\n",
    "        plt.errorbar(grouped_best.index, grouped_best['mean'], yerr=grouped_best['se'], \n",
    "                 fmt='o-', capsize=5, capthick=2, linewidth=2, markersize=8, label=\"Beginning of Sim divergence\")\n",
    "        \n",
    "        plt.xlabel('ε (Openness)', fontsize=14)\n",
    "        plt.ylabel('End of Sim Jensen-Shannon Divergence', fontsize=14)\n",
    "        plt.title(f'Best Fits Only (JS < {best_fit_threshold})\\n{step_titles[stepNo - 1]}', fontsize=16)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(plot_output_dir, f'step{stepNo}_best_final_vs_epsM.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(f\"No good fits found for Step {stepNo}\")\n",
    "        plt.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Additional analysis: Print summary statistics\n",
    "    print(f\"\\nStep {stepNo} Summary Statistics:\")\n",
    "    print(f\"Total simulations: {fit_df['cum_sim_index'].nunique()}\")\n",
    "    print(f\"Total country-year combinations: {fit_df[['country', 'year']].drop_duplicates().shape[0]}\")\n",
    "    print(f\"Mean final distance: {fit_df['distance_final'].mean():.4f}\")\n",
    "    print(f\"Mean initial distance: {fit_df['distance_initial'].mean():.4f}\")\n",
    "    print(f\"Mean delta: {fit_df['distance_delta'].mean():.4f}\")\n",
    "    print(f\"Percentage improved: {(len(improved_fits) / len(fit_df) * 100):.1f}%\")\n",
    "    \n",
    "    # Best epsM values\n",
    "    best_epsM_final = grouped.sort_values('mean').index[0]\n",
    "    best_epsM_delta = grouped_delta.sort_values('mean').index[0]\n",
    "    print(f\"Best epsM for final distance: {best_epsM_final:.3f}\")\n",
    "    print(f\"Best epsM for improvement: {best_epsM_delta:.3f}\")\n",
    "\n",
    "print(f\"\\nAll plots saved to: {plot_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ec44b9-6431-4fe8-be4b-20803cda7d79",
   "metadata": {},
   "source": [
    "## Color Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bf708c-c8a0-4704-b1f1-7f432076cc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create output directory for plots\n",
    "plot_output_dir = os.path.join(plots_folder_path, \"Colorplots\")\n",
    "best_fit_threshold = 0.05\n",
    "\n",
    "# Greek symbols for parameter names\n",
    "param_labels = {\n",
    "    'epsM': r'$\\mu_{\\varepsilon}$',\n",
    "    'epsSD': r'$\\sigma_{\\varepsilon}$', \n",
    "    'OpSD': r'$\\sigma_{InitialOp}$',\n",
    "    'MedM': r'$\\mu_{MediaOp}$',\n",
    "    'MedSD': r'$\\sigma_{MediaOp}$',\n",
    "    'MedInfF': 'Media Influence'\n",
    "}\n",
    "\n",
    "# Global style settings\n",
    "plt.rcParams.update({\n",
    "    'font.size': 14,\n",
    "    'axes.titlesize': 16,\n",
    "    'axes.labelsize': 14,\n",
    "    'xtick.labelsize': 8,\n",
    "    'ytick.labelsize': 12,\n",
    "    'legend.fontsize': 12,\n",
    "    'figure.titlesize': 18\n",
    "})\n",
    "\n",
    "# Global flags\n",
    "SHOW_GRID = True  # Set to True to enable grid\n",
    "\n",
    "# Common color normalization for all plots across all steps\n",
    "def get_global_color_norm(variant, all_steps_data):\n",
    "    \"\"\"Get common color normalization across all steps for a variant\"\"\"\n",
    "    all_values = []\n",
    "    \n",
    "    for step_data in all_steps_data:\n",
    "        if step_data is not None and len(step_data) > 0:\n",
    "            if variant == 'mean_jsd':\n",
    "                values = step_data.groupby(['epsM', 'epsSD'])['distance_final'].mean()\n",
    "            elif variant == 'mean_jsd_improved':\n",
    "                improved = step_data[step_data['distance_delta'] < 0]\n",
    "                values = improved.groupby(['epsM', 'epsSD'])['distance_final'].mean()\n",
    "            elif variant == 'mean_jsd_best_fit':\n",
    "                best_fit = step_data[step_data['distance_final'] < best_fit_threshold]\n",
    "                values = best_fit.groupby(['epsM', 'epsSD'])['distance_final'].mean()\n",
    "            elif variant == 'best_jsd':\n",
    "                values = step_data.groupby(['epsM', 'epsSD'])['distance_final'].min()\n",
    "            \n",
    "            if len(values) > 0:\n",
    "                all_values.extend(values.values)\n",
    "    \n",
    "    if not all_values:\n",
    "        return colors.Normalize(vmin=0, vmax=1)\n",
    "    \n",
    "    return colors.Normalize(vmin=min(all_values), vmax=max(all_values))\n",
    "\n",
    "# Function to create categorical heatmap\n",
    "def create_categorical_heatmap(pivot_table, xlabel, ylabel, title, norm, cmap='viridis_r', ax=None):\n",
    "    \"\"\"Create a categorical heatmap with proper discrete parameter handling\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Get the unique parameter values\n",
    "    x_categories = pivot_table.columns.tolist()\n",
    "    y_categories = pivot_table.index.tolist()\n",
    "    \n",
    "    # Create the heatmap\n",
    "    im = ax.imshow(pivot_table.values, origin='lower', aspect='auto',\n",
    "                   norm=norm, cmap=cmap)\n",
    "    \n",
    "    # Set ticks and labels at cell centers\n",
    "    ax.set_xticks(np.arange(len(x_categories)))\n",
    "    ax.set_yticks(np.arange(len(y_categories)))\n",
    "    ax.set_xticklabels([f\"{x:.2f}\" for x in x_categories])\n",
    "    ax.set_yticklabels([f\"{y:.2f}\" for y in y_categories])\n",
    "    \n",
    "    # Add proper grid lines if enabled\n",
    "    if SHOW_GRID:\n",
    "        # Add grid lines at cell boundaries\n",
    "        ax.set_xticks(np.arange(len(x_categories) + 1) - 0.5, minor=True)\n",
    "        ax.set_yticks(np.arange(len(y_categories) + 1) - 0.5, minor=True)\n",
    "        ax.grid(which=\"minor\", color=\"white\", linestyle='-', linewidth=1.5, alpha=0.8)\n",
    "        ax.tick_params(which=\"minor\", length=0)\n",
    "    \n",
    "    ax.set_xlabel(xlabel, fontsize=14)\n",
    "    ax.set_ylabel(ylabel, fontsize=14)\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    \n",
    "    return im\n",
    "\n",
    "# Function to process data for a variant\n",
    "def process_variant_data(df, variant):\n",
    "    \"\"\"Process data for different variants\"\"\"\n",
    "    if variant == 'mean_jsd':\n",
    "        return df.groupby(['epsM', 'epsSD'])['distance_final'].mean().reset_index()\n",
    "    elif variant == 'mean_jsd_improved':\n",
    "        improved = df[df['distance_delta'] < 0]\n",
    "        return improved.groupby(['epsM', 'epsSD'])['distance_final'].mean().reset_index()\n",
    "    elif variant == 'mean_jsd_best_fit':\n",
    "        best_fit = df[df['distance_final'] < best_fit_threshold]\n",
    "        return best_fit.groupby(['epsM', 'epsSD'])['distance_final'].mean().reset_index()\n",
    "    elif variant == 'best_jsd':\n",
    "        return df.groupby(['epsM', 'epsSD'])['distance_final'].min().reset_index()\n",
    "\n",
    "# Collect all data for global normalization\n",
    "all_steps_data = jsfit_dfs_stepwise[2:5]  # Steps 3, 4, 5\n",
    "\n",
    "# Precompute global norms for each variant\n",
    "global_norms = {}\n",
    "variants = ['mean_jsd', 'mean_jsd_improved', 'mean_jsd_best_fit', 'best_jsd']\n",
    "\n",
    "for variant in variants:\n",
    "    global_norms[variant] = get_global_color_norm(variant, all_steps_data)\n",
    "    print(f\"{variant}: vmin={global_norms[variant].vmin:.4f}, vmax={global_norms[variant].vmax:.4f}\")\n",
    "\n",
    "# Step 3: epsM and epsSD only\n",
    "print(\"Processing Step 3...\")\n",
    "step3_dir = os.path.join(plot_output_dir, \"step3\")\n",
    "os.makedirs(step3_dir, exist_ok=True)\n",
    "\n",
    "step3_data = jsfit_dfs_stepwise[2] if len(jsfit_dfs_stepwise) > 2 else None\n",
    "\n",
    "if step3_data is not None:\n",
    "    # Get unique parameter values for the entire step\n",
    "    unique_epsM = sorted(step3_data['epsM'].unique())\n",
    "    unique_epsSD = sorted(step3_data['epsSD'].unique())\n",
    "    \n",
    "    for variant in variants:\n",
    "        variant_data = process_variant_data(step3_data, variant)\n",
    "        \n",
    "        if len(variant_data) > 0:\n",
    "            # Create pivot table and ensure all parameter combinations are represented\n",
    "            pivot = variant_data.pivot_table(index='epsSD', columns='epsM', values='distance_final')\n",
    "            pivot = pivot.reindex(index=unique_epsSD, columns=unique_epsM)\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(12, 9))\n",
    "            norm = global_norms[variant]\n",
    "            \n",
    "            im = create_categorical_heatmap(pivot, param_labels['epsM'], param_labels['epsSD'],\n",
    "                                          f'Step 3: {variant.replace(\"_\", \" \").title()}', norm, ax=ax)\n",
    "            \n",
    "            # Add colorbar\n",
    "            cbar = plt.colorbar(im, ax=ax)\n",
    "            cbar.set_label('Jensen-Shannon Distance', fontsize=14)\n",
    "            \n",
    "            # Save best_jsd directly in step folder, others in subfolders\n",
    "            if variant == 'best_jsd':\n",
    "                plt.savefig(os.path.join(step3_dir, f'step3_{variant}.png'), \n",
    "                           dpi=300, bbox_inches='tight')\n",
    "            else:\n",
    "                variant_dir = os.path.join(step3_dir, variant)\n",
    "                os.makedirs(variant_dir, exist_ok=True)\n",
    "                plt.savefig(os.path.join(variant_dir, f'step3_{variant}.png'), \n",
    "                           dpi=300, bbox_inches='tight')\n",
    "            \n",
    "            plt.close()\n",
    "\n",
    "# Step 4: epsM, epsSD, and OpSD\n",
    "print(\"Processing Step 4...\")\n",
    "step4_dir = os.path.join(plot_output_dir, \"step4\")\n",
    "os.makedirs(step4_dir, exist_ok=True)\n",
    "\n",
    "step4_data = jsfit_dfs_stepwise[3] if len(jsfit_dfs_stepwise) > 3 else None\n",
    "\n",
    "if step4_data is not None:\n",
    "    # Get unique parameter values for the entire step\n",
    "    unique_epsM = sorted(step4_data['epsM'].unique())\n",
    "    unique_epsSD = sorted(step4_data['epsSD'].unique())\n",
    "    unique_OpSD = sorted(step4_data['OpSD'].unique())\n",
    "    \n",
    "    for variant in variants:\n",
    "        for OpSD_val in unique_OpSD:\n",
    "            OpSD_data = step4_data[step4_data['OpSD'] == OpSD_val]\n",
    "            variant_data = process_variant_data(OpSD_data, variant)\n",
    "            \n",
    "            if len(variant_data) > 0:\n",
    "                # Create pivot table and ensure all parameter combinations are represented\n",
    "                pivot = variant_data.pivot_table(index='epsSD', columns='epsM', values='distance_final')\n",
    "                pivot = pivot.reindex(index=unique_epsSD, columns=unique_epsM)\n",
    "                \n",
    "                fig, ax = plt.subplots(figsize=(12, 9))\n",
    "                norm = global_norms[variant]\n",
    "                \n",
    "                im = create_categorical_heatmap(pivot, param_labels['epsM'], param_labels['epsSD'],\n",
    "                                              f'Step 4: {variant.replace(\"_\", \" \").title()}\\n{param_labels[\"OpSD\"]} = {OpSD_val:.2f}',\n",
    "                                              norm, ax=ax)\n",
    "                \n",
    "                # Add colorbar\n",
    "                cbar = plt.colorbar(im, ax=ax)\n",
    "                cbar.set_label('Jensen-Shannon Distance', fontsize=14)\n",
    "                \n",
    "                # Save best_jsd directly in step folder, others in subfolders\n",
    "                if variant == 'best_jsd':\n",
    "                    plt.savefig(os.path.join(step4_dir, f'step4_OpSD_{OpSD_val:.2f}_{variant}.png'), \n",
    "                               dpi=300, bbox_inches='tight')\n",
    "                else:\n",
    "                    variant_dir = os.path.join(step4_dir, variant)\n",
    "                    os.makedirs(variant_dir, exist_ok=True)\n",
    "                    plt.savefig(os.path.join(variant_dir, f'step4_OpSD_{OpSD_val:.2f}_{variant}.png'), \n",
    "                               dpi=300, bbox_inches='tight')\n",
    "                \n",
    "                plt.close()\n",
    "\n",
    "# Step 5: epsM, epsSD, and OpSD with media\n",
    "print(\"Processing Step 5...\")\n",
    "step5_dir = os.path.join(plot_output_dir, \"step5\")\n",
    "os.makedirs(step5_dir, exist_ok=True)\n",
    "\n",
    "step5_data = jsfit_dfs_stepwise[4] if len(jsfit_dfs_stepwise) > 4 else None\n",
    "\n",
    "if step5_data is not None:\n",
    "    # Get unique parameter values for the entire step\n",
    "    unique_epsM = sorted(step5_data['epsM'].unique())\n",
    "    unique_epsSD = sorted(step5_data['epsSD'].unique())\n",
    "    unique_MedM = sorted(step5_data['MedM'].unique())\n",
    "    unique_MedSD = sorted(step5_data['MedSD'].unique())\n",
    "    unique_MedInfF = sorted(step5_data['MedInfF'].unique())\n",
    "    \n",
    "    for variant in variants:\n",
    "        for MedInfF_val in unique_MedInfF:\n",
    "            MedInfF_data = step5_data[step5_data['MedInfF'] == MedInfF_val]\n",
    "            \n",
    "            # Create subplot grid\n",
    "            fig, axes = plt.subplots(len(unique_MedSD), len(unique_MedM), \n",
    "                                    figsize=(6*len(unique_MedM), 5*len(unique_MedSD)),\n",
    "                                    sharex=True, sharey=True)\n",
    "            \n",
    "            if len(unique_MedSD) == 1 and len(unique_MedM) == 1:\n",
    "                axes = np.array([[axes]])\n",
    "            elif len(unique_MedSD) == 1:\n",
    "                axes = axes[np.newaxis, :]\n",
    "            elif len(unique_MedM) == 1:\n",
    "                axes = axes[:, np.newaxis]\n",
    "            \n",
    "            # Get global norm for this variant\n",
    "            norm = global_norms[variant]\n",
    "            \n",
    "            # Find the first valid subplot to create colorbar\n",
    "            first_valid_im = None\n",
    "            \n",
    "            for i, MedSD_val in enumerate(unique_MedSD):\n",
    "                for j, MedM_val in enumerate(unique_MedM):\n",
    "                    ax = axes[i, j]\n",
    "                    \n",
    "                    # Filter data for this parameter combination\n",
    "                    param_data = MedInfF_data[\n",
    "                        (MedInfF_data['MedSD'] == MedSD_val) & \n",
    "                        (MedInfF_data['MedM'] == MedM_val)\n",
    "                    ]\n",
    "                    \n",
    "                    if len(param_data) > 0:\n",
    "                        variant_data = process_variant_data(param_data, variant)\n",
    "                        if len(variant_data) > 0:\n",
    "                            # Create pivot table and ensure all parameter combinations are represented\n",
    "                            pivot = variant_data.pivot_table(index='epsSD', columns='epsM', values='distance_final')\n",
    "                            pivot = pivot.reindex(index=unique_epsSD, columns=unique_epsM)\n",
    "                            \n",
    "                            im = create_categorical_heatmap(pivot, '', '', '', norm, ax=ax)\n",
    "                            if first_valid_im is None:\n",
    "                                first_valid_im = im\n",
    "                            \n",
    "                            # Add parameter labels to subplots\n",
    "                            if i == len(unique_MedSD) - 1:\n",
    "                                ax.set_xlabel(param_labels['epsM'], fontsize=12)\n",
    "                            if j == 0:\n",
    "                                ax.set_ylabel(param_labels['epsSD'], fontsize=12)\n",
    "                            \n",
    "                            # Add MedM and MedSD values to subplot title\n",
    "                            ax.set_title(f'{param_labels[\"MedM\"]}={MedM_val:.2f}\\n{param_labels[\"MedSD\"]}={MedSD_val:.2f}', \n",
    "                                       fontsize=11)\n",
    "                    else:\n",
    "                        ax.axis('off')\n",
    "            \n",
    "            # Add overall title\n",
    "            fig.suptitle(f'Step 5: {variant.replace(\"_\", \" \").title()}\\n'\n",
    "                        f'{param_labels[\"MedInfF\"]} = {MedInfF_val:.2f}', \n",
    "                        fontsize=18, y=0.95)\n",
    "            \n",
    "            # Add single colorbar for the entire figure\n",
    "            if first_valid_im is not None:\n",
    "                cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "                cbar = fig.colorbar(first_valid_im, cax=cbar_ax)\n",
    "                cbar.set_label('Jensen-Shannon Distance', fontsize=14)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.subplots_adjust(top=0.9, bottom=0.1, right=0.9)\n",
    "            \n",
    "            # Save best_jsd directly in step folder, others in subfolders\n",
    "            if variant == 'best_jsd':\n",
    "                plt.savefig(os.path.join(step5_dir, f'step5_MedInfF_{MedInfF_val:.2f}_{variant}.png'), \n",
    "                           dpi=300, bbox_inches='tight')\n",
    "            else:\n",
    "                variant_dir = os.path.join(step5_dir, variant)\n",
    "                os.makedirs(variant_dir, exist_ok=True)\n",
    "                plt.savefig(os.path.join(variant_dir, f'step5_MedInfF_{MedInfF_val:.2f}_{variant}.png'), \n",
    "                           dpi=300, bbox_inches='tight')\n",
    "            \n",
    "            plt.close()\n",
    "\n",
    "print(\"All color plots generated successfully!\")\n",
    "print(f\"Plots saved to: {plot_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5274dc4f-55b5-49f7-b740-714bd8e4d147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test block for Step 5 with categorical treatment of parameters\n",
    "def create_categorical_heatmap(pivot_table, xlabel, ylabel, title, norm, cmap='viridis_r', ax=None):\n",
    "    \"\"\"Create a categorical heatmap with proper discrete parameter handling\"\"\"\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Get the unique parameter values\n",
    "    x_categories = pivot_table.columns.tolist()\n",
    "    y_categories = pivot_table.index.tolist()\n",
    "\n",
    "    # Add grid lines between cells\n",
    "    ax.set_xticks(np.arange(len(x_categories)+1)-0.5, minor=True)\n",
    "    ax.set_yticks(np.arange(len(y_categories)+1)-0.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"white\", linestyle='-', linewidth=1)\n",
    "    ax.tick_params(which=\"minor\", size=0)\n",
    "\n",
    "        \n",
    "    # Create the heatmap\n",
    "    im = ax.imshow(pivot_table.values, origin='lower', aspect='auto',\n",
    "                   norm=norm, cmap=cmap)\n",
    "    \n",
    "    # Set ticks and labels\n",
    "    ax.set_xticks(np.arange(len(x_categories)))\n",
    "    ax.set_yticks(np.arange(len(y_categories)))\n",
    "    ax.set_xticklabels([f\"{x:.2f}\" for x in x_categories])\n",
    "    ax.set_yticklabels([f\"{y:.2f}\" for y in y_categories])\n",
    "    \n",
    "    ax.set_xlabel(xlabel, fontsize=14)\n",
    "    ax.set_ylabel(ylabel, fontsize=14)\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    \n",
    "    return im\n",
    "\n",
    "# Test with a specific variant and MedInfF value from Step 5\n",
    "test_variant = 'best_jsd'\n",
    "test_MedInfF = step5_data['MedInfF'].iloc[0]  # Use the first value for testing\n",
    "\n",
    "# Get unique parameter values for the entire step\n",
    "unique_epsM = sorted(step5_data['epsM'].unique())\n",
    "unique_epsSD = sorted(step5_data['epsSD'].unique())\n",
    "unique_MedM = sorted(step5_data['MedM'].unique())\n",
    "unique_MedSD = sorted(step5_data['MedSD'].unique())\n",
    "\n",
    "# Filter data for the test MedInfF value\n",
    "test_data = step5_data[step5_data['MedInfF'] == test_MedInfF]\n",
    "\n",
    "# Create subplot grid\n",
    "fig, axes = plt.subplots(len(unique_MedSD), len(unique_MedM), \n",
    "                        figsize=(6*len(unique_MedM), 5*len(unique_MedSD)),\n",
    "                        sharex=True, sharey=True)\n",
    "\n",
    "if len(unique_MedSD) == 1 and len(unique_MedM) == 1:\n",
    "    axes = np.array([[axes]])\n",
    "elif len(unique_MedSD) == 1:\n",
    "    axes = axes[np.newaxis, :]\n",
    "elif len(unique_MedM) == 1:\n",
    "    axes = axes[:, np.newaxis]\n",
    "\n",
    "# Get global norm for this variant\n",
    "norm = global_norms[test_variant]\n",
    "\n",
    "# Find the first valid subplot to create colorbar\n",
    "first_valid_im = None\n",
    "\n",
    "for i, MedSD_val in enumerate(unique_MedSD):\n",
    "    for j, MedM_val in enumerate(unique_MedM):\n",
    "        ax = axes[i, j]\n",
    "        \n",
    "        # Filter data for this parameter combination\n",
    "        param_data = test_data[\n",
    "            (test_data['MedSD'] == MedSD_val) & \n",
    "            (test_data['MedM'] == MedM_val)\n",
    "        ]\n",
    "        \n",
    "        if len(param_data) > 0:\n",
    "            variant_data = process_variant_data(param_data, test_variant)\n",
    "            if len(variant_data) > 0:\n",
    "                # Create pivot table and ensure all parameter combinations are represented\n",
    "                pivot = variant_data.pivot_table(index='epsSD', columns='epsM', values='distance_final')\n",
    "                \n",
    "                # Reindex to include all possible parameter values\n",
    "                pivot = pivot.reindex(index=unique_epsSD, columns=unique_epsM)\n",
    "                \n",
    "                im = create_categorical_heatmap(pivot, '', '', '', norm, ax=ax)\n",
    "                if first_valid_im is None:\n",
    "                    first_valid_im = im\n",
    "                \n",
    "                # Add parameter labels to subplots\n",
    "                if i == len(unique_MedSD) - 1:\n",
    "                    ax.set_xlabel(param_labels['epsM'], fontsize=12)\n",
    "                if j == 0:\n",
    "                    ax.set_ylabel(param_labels['epsSD'], fontsize=12)\n",
    "                \n",
    "                # Add MedM and MedSD values to subplot title\n",
    "                ax.set_title(f'{param_labels[\"MedM\"]}={MedM_val:.2f}\\n{param_labels[\"MedSD\"]}={MedSD_val:.2f}', \n",
    "                           fontsize=11)\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "\n",
    "# Add overall title\n",
    "fig.suptitle(f'Step 5: {test_variant.replace(\"_\", \" \").title()}\\n'\n",
    "            f'{param_labels[\"MedInfF\"]} = {test_MedInfF:.2f}', \n",
    "            fontsize=18, y=0.95)\n",
    "\n",
    "# Add single colorbar for the entire figure\n",
    "if first_valid_im is not None:\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "    cbar = fig.colorbar(first_valid_im, cax=cbar_ax)\n",
    "    cbar.set_label('Jensen-Shannon Distance', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9, bottom=0.1, right=0.9)\n",
    "\n",
    "# Save the test plot\n",
    "test_dir = os.path.join(plot_output_dir, \"test\")\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "plt.savefig(os.path.join(test_dir, f'step5_test_{test_variant}_MedInfF_{test_MedInfF:.2f}.png'), \n",
    "           dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"Test plot created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f040af-77bb-4865-b302-0976ace6e95a",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5804f3-c426-4732-b950-8d1a830ba9ec",
   "metadata": {},
   "source": [
    "# Step 6 - Media with Silence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c2d9a8-ec64-4413-9454-c1a9ece95281",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsfit_dfs_stepwise[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db17c42-93db-4b81-b898-89ac82626172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import colors\n",
    "\n",
    "# Focus only on Step 6\n",
    "step6_data = jsfit_dfs_stepwise[5] if len(jsfit_dfs_stepwise) > 5 else None\n",
    "\n",
    "if step6_data is not None:\n",
    "    # Set up output directory\n",
    "    plot_output_dir = os.path.join(plots_folder_path, \"Colorplots_Step6_Percentage\")\n",
    "    os.makedirs(plot_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get unique parameter values for Step 6\n",
    "    unique_epsM = sorted(step6_data['epsM'].unique())\n",
    "    unique_epsSD = sorted(step6_data['epsSD'].unique())\n",
    "    unique_MedM = sorted(step6_data['MedM'].unique())\n",
    "    unique_MedSD = sorted(step6_data['MedSD'].unique())\n",
    "    unique_Silence_Alpha = sorted(step6_data['Silence_Alpha'].dropna().unique())\n",
    "    unique_Silence_Tau = sorted(step6_data['Silence_Tau'].dropna().unique())\n",
    "    unique_Silence_Delta0 = sorted(step6_data['Silence_Delta0'].dropna().unique())\n",
    "    unique_SilenceByBoundary = sorted(step6_data['SilenceByBoundary'].dropna().unique())\n",
    "    \n",
    "    print(f\"Step 6 Parameter Ranges:\")\n",
    "    print(f\"  epsM: {unique_epsM}\")\n",
    "    print(f\"  epsSD: {unique_epsSD}\")\n",
    "    print(f\"  MedM: {unique_MedM}\")\n",
    "    print(f\"  MedSD: {unique_MedSD}\")\n",
    "    print(f\"  Silence_Alpha: {unique_Silence_Alpha}\")\n",
    "    print(f\"  Silence_Tau: {unique_Silence_Tau}\")\n",
    "    print(f\"  Silence_Delta0: {unique_Silence_Delta0}\")\n",
    "    print(f\"  SilenceByBoundary: {unique_SilenceByBoundary}\")\n",
    "    \n",
    "    # Parameter labels\n",
    "    param_labels = {\n",
    "        'epsM': r'$\\mu_{\\varepsilon}$',\n",
    "        'epsSD': r'$\\sigma_{\\varepsilon}$', \n",
    "        'MedM': r'$\\mu_{MediaOp}$',\n",
    "        'MedSD': r'$\\sigma_{MediaOp}$',\n",
    "        'Silence_Alpha': r'$\\alpha_{Silence}$',\n",
    "        'Silence_Tau': r'$\\tau_{Silence}$',\n",
    "        'Silence_Delta0': r'$\\delta_0_{Silence}$',\n",
    "        'SilenceByBoundary': 'SilenceByBoundary'\n",
    "    }\n",
    "    \n",
    "    def calculate_percentage_below_threshold(df):\n",
    "        \"\"\"Calculate percentage of simulations below threshold for each epsM/epsSD combination\"\"\"\n",
    "        if len(df) == 0:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        result = df.groupby(['epsM', 'epsSD']).apply(\n",
    "            lambda x: (x['distance_final'] <= best_fit_threshold).mean() * 100\n",
    "        ).reset_index()\n",
    "        result.columns = ['epsM', 'epsSD', 'pct_below_threshold']\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def create_categorical_heatmap(pivot_table, xlabel, ylabel, title, ax):\n",
    "        \"\"\"Create a categorical heatmap for percentage below threshold\"\"\"\n",
    "        # Get the unique parameter values\n",
    "        x_categories = pivot_table.columns.tolist()\n",
    "        y_categories = pivot_table.index.tolist()\n",
    "        \n",
    "        # Create the heatmap with percentage colormap\n",
    "        im = ax.imshow(pivot_table.values, origin='lower', aspect='auto',\n",
    "                      cmap='RdYlBu', vmin=0, vmax=100)\n",
    "        \n",
    "        # Set ticks and labels at cell centers\n",
    "        ax.set_xticks(np.arange(len(x_categories)))\n",
    "        ax.set_yticks(np.arange(len(y_categories)))\n",
    "        ax.set_xticklabels([f\"{x:.2f}\" for x in x_categories], fontsize=8)\n",
    "        ax.set_yticklabels([f\"{y:.2f}\" for y in y_categories], fontsize=8)\n",
    "        \n",
    "        # Add value annotations\n",
    "        for i in range(len(y_categories)):\n",
    "            for j in range(len(x_categories)):\n",
    "                value = pivot_table.iloc[i, j]\n",
    "                if not np.isnan(value):\n",
    "                    ax.text(j, i, f'{value:.0f}%', \n",
    "                           ha='center', va='center', \n",
    "                           fontsize=7, fontweight='bold',\n",
    "                           color='white' if value > 50 else 'black')\n",
    "        \n",
    "        ax.set_xlabel(xlabel, fontsize=10)\n",
    "        ax.set_ylabel(ylabel, fontsize=10)\n",
    "        ax.set_title(title, fontsize=11)\n",
    "        \n",
    "        return im\n",
    "    \n",
    "    # Generate plots for all silence parameter combinations\n",
    "    total_combinations = (len(unique_Silence_Alpha) * len(unique_Silence_Tau) * \n",
    "                         len(unique_Silence_Delta0) * len(unique_SilenceByBoundary))\n",
    "    current_combination = 0\n",
    "    \n",
    "    for silence_alpha in unique_Silence_Alpha:\n",
    "        for silence_tau in unique_Silence_Tau:\n",
    "            for silence_delta0 in unique_Silence_Delta0:\n",
    "                for silence_boundary in unique_SilenceByBoundary:\n",
    "                    current_combination += 1\n",
    "                    print(f\"Processing combination {current_combination}/{total_combinations}: \"\n",
    "                          f\"SA={silence_alpha}, ST={silence_tau}, SDO={silence_delta0}, SBB={silence_boundary}\")\n",
    "                    \n",
    "                    # Filter data for this silence parameter combination\n",
    "                    silence_data = step6_data[\n",
    "                        (step6_data['Silence_Alpha'] == silence_alpha) &\n",
    "                        (step6_data['Silence_Tau'] == silence_tau) &\n",
    "                        (step6_data['Silence_Delta0'] == silence_delta0) &\n",
    "                        (step6_data['SilenceByBoundary'] == silence_boundary)\n",
    "                    ]\n",
    "                    \n",
    "                    if len(silence_data) == 0:\n",
    "                        print(f\"  No data for this combination, skipping...\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Create subplot grid for Media parameters\n",
    "                    fig, axes = plt.subplots(len(unique_MedSD), len(unique_MedM), \n",
    "                                            figsize=(4*len(unique_MedM), 3.5*len(unique_MedSD)))\n",
    "                    \n",
    "                    if len(unique_MedSD) == 1 and len(unique_MedM) == 1:\n",
    "                        axes = np.array([[axes]])\n",
    "                    elif len(unique_MedSD) == 1:\n",
    "                        axes = axes[np.newaxis, :]\n",
    "                    elif len(unique_MedM) == 1:\n",
    "                        axes = axes[:, np.newaxis]\n",
    "                    \n",
    "                    # Find the first valid subplot to create colorbar\n",
    "                    first_valid_im = None\n",
    "                    \n",
    "                    for i, med_sd in enumerate(unique_MedSD):\n",
    "                        for j, med_m in enumerate(unique_MedM):\n",
    "                            ax = axes[i, j]\n",
    "                            \n",
    "                            # Filter data for this media parameter combination\n",
    "                            media_data = silence_data[\n",
    "                                (silence_data['MedSD'] == med_sd) & \n",
    "                                (silence_data['MedM'] == med_m)\n",
    "                            ]\n",
    "                            \n",
    "                            if len(media_data) > 0:\n",
    "                                # Calculate percentage below threshold\n",
    "                                percentage_data = calculate_percentage_below_threshold(media_data)\n",
    "                                \n",
    "                                if len(percentage_data) > 0:\n",
    "                                    # Create pivot table\n",
    "                                    pivot = percentage_data.pivot_table(index='epsSD', columns='epsM', \n",
    "                                                                       values='pct_below_threshold')\n",
    "                                    pivot = pivot.reindex(index=unique_epsSD, columns=unique_epsM)\n",
    "                                    \n",
    "                                    im = create_categorical_heatmap(pivot, '', '', '', ax)\n",
    "                                    if first_valid_im is None:\n",
    "                                        first_valid_im = im\n",
    "                                    \n",
    "                                    # Add Media parameter labels to subplots\n",
    "                                    if i == len(unique_MedSD) - 1:\n",
    "                                        ax.set_xlabel(param_labels['epsM'], fontsize=9)\n",
    "                                    if j == 0:\n",
    "                                        ax.set_ylabel(param_labels['epsSD'], fontsize=9)\n",
    "                                    \n",
    "                                    # Add MedM and MedSD values to subplot title\n",
    "                                    ax.set_title(f'{param_labels[\"MedM\"]}={med_m:.2f}\\n{param_labels[\"MedSD\"]}={med_sd:.2f}', \n",
    "                                               fontsize=9)\n",
    "                            else:\n",
    "                                ax.axis('off')\n",
    "                                ax.text(0.5, 0.5, 'No Data', \n",
    "                                       ha='center', va='center', transform=ax.transAxes, fontsize=10)\n",
    "                    \n",
    "                    # Create comprehensive title with all parameters\n",
    "                    title_params = [\n",
    "                        f\"{param_labels['Silence_Alpha']}={silence_alpha:.2f}\",\n",
    "                        f\"{param_labels['Silence_Tau']}={silence_tau:.2f}\",\n",
    "                        f\"{param_labels['Silence_Delta0']}={silence_delta0:.2f}\",\n",
    "                        f\"{param_labels['SilenceByBoundary']}={silence_boundary}\"\n",
    "                    ]\n",
    "                    \n",
    "                    fig.suptitle(f'Step 6: Percentage of Simulations with JSD ≤ {best_fit_threshold}\\n' +\n",
    "                                ' | '.join(title_params), \n",
    "                                fontsize=14, y=0.98)\n",
    "                    \n",
    "                    # Add single colorbar for the entire figure\n",
    "                    if first_valid_im is not None:\n",
    "                        cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "                        cbar = fig.colorbar(first_valid_im, cax=cbar_ax)\n",
    "                        cbar.set_label('Percentage Below Threshold (%)', fontsize=10)\n",
    "                    \n",
    "                    # Create filename with all parameters\n",
    "                    filename_parts = [\n",
    "                        f\"step6\",\n",
    "                        f\"SA{silence_alpha:.2f}\",\n",
    "                        f\"ST{silence_tau:.2f}\",\n",
    "                        f\"SDO{silence_delta0:.2f}\",\n",
    "                        f\"SBB{silence_boundary}\"\n",
    "                    ]\n",
    "                    filename = \"_\".join(filename_parts) + \".png\"\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.subplots_adjust(top=0.92, bottom=0.1, right=0.9)\n",
    "                    plt.savefig(os.path.join(plot_output_dir, filename), \n",
    "                               dpi=300, bbox_inches='tight')\n",
    "                    plt.close()\n",
    "                    \n",
    "                    print(f\"  Saved: {filename}\")\n",
    "    \n",
    "    print(f\"\\nStep 6 colorplots completed! Saved to: {plot_output_dir}\")\n",
    "    \n",
    "    # Generate a summary statistics file\n",
    "    summary_stats = []\n",
    "    for silence_alpha in unique_Silence_Alpha:\n",
    "        for silence_tau in unique_Silence_Tau:\n",
    "            for silence_delta0 in unique_Silence_Delta0:\n",
    "                for silence_boundary in unique_SilenceByBoundary:\n",
    "                    silence_data = step6_data[\n",
    "                        (step6_data['Silence_Alpha'] == silence_alpha) &\n",
    "                        (step6_data['Silence_Tau'] == silence_tau) &\n",
    "                        (step6_data['Silence_Delta0'] == silence_delta0) &\n",
    "                        (step6_data['SilenceByBoundary'] == silence_boundary)\n",
    "                    ]\n",
    "                    \n",
    "                    if len(silence_data) > 0:\n",
    "                        overall_percentage = (silence_data['distance_final'] <= best_fit_threshold).mean() * 100\n",
    "                        summary_stats.append({\n",
    "                            'Silence_Alpha': silence_alpha,\n",
    "                            'Silence_Tau': silence_tau,\n",
    "                            'Silence_Delta0': silence_delta0,\n",
    "                            'SilenceByBoundary': silence_boundary,\n",
    "                            'Overall_Percentage': overall_percentage,\n",
    "                            'Total_Simulations': len(silence_data)\n",
    "                        })\n",
    "    \n",
    "    if summary_stats:\n",
    "        summary_df = pd.DataFrame(summary_stats)\n",
    "        summary_df.to_csv(os.path.join(plot_output_dir, \"step6_summary_statistics.csv\"), index=False)\n",
    "        print(f\"Summary statistics saved to: step6_summary_statistics.csv\")\n",
    "\n",
    "else:\n",
    "    print(\"Step 6 data not found in jsfit_dfs_stepwise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1a1df2-0bd7-4f0f-b856-905e31319c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "step6_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbc7fed-7cc0-4f8a-bc8e-b2dcfb5d7dae",
   "metadata": {},
   "source": [
    "# FIT SUMMARY ACROSS STEPS BROKEN DOWN BY PARAMERTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60cfc33-88dc-442e-af1e-b31a000601ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up the output directory structure\n",
    "plot_output_dir = os.path.join(plots_folder_path, \"Percent_Fit_By_Param\")\n",
    "threshold_folder = f\"threshold_{best_fit_threshold:.2f}\".replace('.', 'dot')\n",
    "threshold_dir = os.path.join(plot_output_dir, threshold_folder)\n",
    "os.makedirs(threshold_dir, exist_ok=True)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"tab10\")\n",
    "\n",
    "# Define parameters for each step\n",
    "step_parameters = {\n",
    "    1: ['epsM'],\n",
    "    2: ['epsM'],\n",
    "    3: ['epsM', 'epsSD'],\n",
    "    4: ['epsM', 'epsSD', 'OpSD'],\n",
    "    5: ['epsM', 'epsSD', 'MedM', 'MedSD', 'MedInfF'],\n",
    "    6: ['epsM', 'epsSD', 'MedM', 'MedSD', 'Silence_Alpha', 'Silence_Tau', 'Silence_Delta0', 'SilenceByBoundary']\n",
    "}\n",
    "\n",
    "# Parameter labels for plotting\n",
    "param_labels = {\n",
    "    'epsM': r'$\\mu_{\\varepsilon}$',\n",
    "    'epsSD': r'$\\sigma_{\\varepsilon}$', \n",
    "    'OpSD': r'$\\sigma_{InitialOp}$',\n",
    "    'MedM': r'$\\mu_{MediaOp}$',\n",
    "    'MedSD': r'$\\sigma_{MediaOp}$',\n",
    "    'MedInfF': 'Media Influence',\n",
    "    'Silence_Alpha': r'$\\alpha_{Silence}$',\n",
    "    'Silence_Tau': r'$\\tau_{Silence}$',\n",
    "    'Silence_Delta0': r'$\\delta0_{Silence}$',\n",
    "    'SilenceByBoundary': 'Silence by Boundary'\n",
    "}\n",
    "\n",
    "def calculate_fit_percentage_by_param(step_data, param_name):\n",
    "    \"\"\"Calculate percentage of simulations that fit for each value of a parameter\"\"\"\n",
    "    if param_name not in step_data.columns:\n",
    "        return None\n",
    "    \n",
    "    # Group by parameter value and calculate fit percentage\n",
    "    result = step_data.groupby(param_name).apply(\n",
    "        lambda x: (x['distance_final'] <= best_fit_threshold).mean() * 100\n",
    "    ).reset_index()\n",
    "    result.columns = [param_name, 'fit_percentage']\n",
    "    \n",
    "    # Also calculate number of simulations per parameter value\n",
    "    counts = step_data.groupby(param_name).size().reset_index(name='count')\n",
    "    result = result.merge(counts, on=param_name)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def create_parameter_plot(step_data, param_name, step_no, ax):\n",
    "    \"\"\"Create a plot showing fit percentage for a specific parameter\"\"\"\n",
    "    data = calculate_fit_percentage_by_param(step_data, param_name)\n",
    "    \n",
    "    if data is None or len(data) == 0:\n",
    "        ax.text(0.5, 0.5, f'No data for {param_name}', \n",
    "                ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title(f\"{param_labels.get(param_name, param_name)} - No Data\", fontsize=14)\n",
    "        return\n",
    "    \n",
    "    # Sort by parameter value for better visualization\n",
    "    data = data.sort_values(param_name)\n",
    "    \n",
    "    # Create the plot\n",
    "    if data[param_name].dtype in [np.float64, np.int64]:\n",
    "        # Numerical parameter - use line plot\n",
    "        ax.plot(data[param_name], data['fit_percentage'], \n",
    "                marker='o', linewidth=2, markersize=6, markerfacecolor='white')\n",
    "        ax.set_xlabel(param_labels.get(param_name, param_name), fontsize=12)\n",
    "    else:\n",
    "        # Categorical parameter - use bar plot\n",
    "        x_pos = np.arange(len(data))\n",
    "        bars = ax.bar(x_pos, data['fit_percentage'], alpha=0.7)\n",
    "        ax.set_xlabel(param_labels.get(param_name, param_name), fontsize=12)\n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels([f'{x:.2f}' if isinstance(x, (int, float)) else str(x) \n",
    "                           for x in data[param_name]], rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, percentage, count in zip(bars, data['fit_percentage'], data['count']):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                   f'{percentage:.1f}%\\n(n={count})', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    ax.set_ylabel('Fit Percentage (%)', fontsize=12)\n",
    "    # ax.set_ylim(0, 105)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add threshold line and better title\n",
    "    # ax.axhline(y=50, color='red', linestyle='--', alpha=0.5, label='50% threshold')\n",
    "    ax.set_title(f\"{param_labels.get(param_name, param_name)}\\nStep {step_no}\", fontsize=14)\n",
    "    \n",
    "    # Add legend for numerical plots\n",
    "    if data[param_name].dtype in [np.float64, np.int64]:\n",
    "        ax.legend()\n",
    "\n",
    "# Process each step\n",
    "for step_idx in range(6):\n",
    "    step_no = step_idx + 1\n",
    "    step_data = jsfit_dfs_stepwise[step_idx]\n",
    "    \n",
    "    if step_data is None or len(step_data) == 0:\n",
    "        print(f\"Skipping Step {step_no} - no data\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Processing Step {step_no}...\")\n",
    "    \n",
    "    # Create step directory\n",
    "    step_dir = os.path.join(threshold_dir, f\"step{step_no}\")\n",
    "    os.makedirs(step_dir, exist_ok=True)\n",
    "    \n",
    "    # Get parameters for this step\n",
    "    if step_no in step_parameters:\n",
    "        params = step_parameters[step_no]\n",
    "    else:\n",
    "        # Fallback: use all available parameters except distance columns and indices\n",
    "        exclude_cols = ['distance_final', 'distance_initial', 'distance_delta', \n",
    "                       'sim_index', 'country', 'year', 'survey_index']\n",
    "        params = [col for col in step_data.columns if col not in exclude_cols \n",
    "                 and step_data[col].notna().any()]\n",
    "    \n",
    "    print(f\"  Parameters to analyze: {params}\")\n",
    "    \n",
    "    # Calculate overall fit percentage for this step\n",
    "    overall_fit_percentage = (step_data['distance_final'] <= best_fit_threshold).mean() * 100\n",
    "    total_simulations = len(step_data)\n",
    "    \n",
    "    print(f\"  Overall fit percentage: {overall_fit_percentage:.1f}%\")\n",
    "    print(f\"  Total simulations: {total_simulations}\")\n",
    "    \n",
    "    # Create individual plots for each parameter\n",
    "    for param in params:\n",
    "        if param not in step_data.columns:\n",
    "            print(f\"    Skipping {param} - not in data\")\n",
    "            continue\n",
    "        \n",
    "        # Create individual plot for this parameter\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        create_parameter_plot(step_data, param, step_no, ax)\n",
    "        \n",
    "        # Add overall statistics to the plot\n",
    "        fig.text(0.02, 0.02, \n",
    "                f'Overall fit: {overall_fit_percentage:.1f}% | Total simulations: {total_simulations} | Threshold: {best_fit_threshold}',\n",
    "                fontsize=10, style='italic')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(step_dir, f\"step{step_no}_{param}_fit_percentage.png\"), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # Create a summary figure with all parameters for this step\n",
    "    if len(params) > 0:\n",
    "        n_cols = min(3, len(params))\n",
    "        n_rows = (len(params) + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "        \n",
    "        if n_rows == 1 and n_cols == 1:\n",
    "            axes = np.array([axes])\n",
    "        elif n_rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        elif n_cols == 1:\n",
    "            axes = axes.reshape(-1, 1)\n",
    "        \n",
    "        # Flatten axes array for easy iteration\n",
    "        axes_flat = axes.flatten()\n",
    "        \n",
    "        for i, param in enumerate(params):\n",
    "            if i < len(axes_flat):\n",
    "                create_parameter_plot(step_data, param, step_no, axes_flat[i])\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for i in range(len(params), len(axes_flat)):\n",
    "            axes_flat[i].axis('off')\n",
    "        \n",
    "        # Add overall title\n",
    "        fig.suptitle(f'Step {step_no}: Fit Percentage by Parameter\\n(Threshold = {best_fit_threshold})', \n",
    "                    fontsize=16, y=0.98)\n",
    "        \n",
    "        # Add overall statistics\n",
    "        fig.text(0.02, 0.02, \n",
    "                f'Overall fit percentage: {overall_fit_percentage:.1f}% | Total simulations: {total_simulations}',\n",
    "                fontsize=10, style='italic')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.92, bottom=0.08)\n",
    "        plt.savefig(os.path.join(step_dir, f\"step{step_no}_summary_fit_percentage.png\"), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "print(f\"\\nAnalysis complete! Plots saved to: {threshold_dir}\")\n",
    "\n",
    "# Create a comprehensive summary across all steps\n",
    "print(\"\\n=== COMPREHENSIVE SUMMARY ACROSS ALL STEPS ===\")\n",
    "summary_data = []\n",
    "\n",
    "for step_idx in range(len(jsfit_dfs_stepwise)):\n",
    "    step_no = step_idx + 1\n",
    "    step_data = jsfit_dfs_stepwise[step_idx]\n",
    "    \n",
    "    if step_data is None or len(step_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    fit_percentage = (step_data['distance_final'] <= best_fit_threshold).mean() * 100\n",
    "    total_sims = len(step_data)\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Step': step_no,\n",
    "        'Fit_Percentage': fit_percentage,\n",
    "        'Total_Simulations': total_sims,\n",
    "        'Parameters': ', '.join(step_parameters.get(step_no, ['Unknown']))\n",
    "    })\n",
    "\n",
    "if summary_data:\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(\"\\nStep Summary:\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # Create summary plot across steps\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Fit percentage by step\n",
    "    ax1.bar(summary_df['Step'], summary_df['Fit_Percentage'], color='skyblue', alpha=0.7)\n",
    "    ax1.set_xlabel('Step', fontsize=12)\n",
    "    ax1.set_ylabel('Fit Percentage (%)', fontsize=12)\n",
    "    ax1.set_title('Fit Percentage by Step', fontsize=14)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (step, percentage) in enumerate(zip(summary_df['Step'], summary_df['Fit_Percentage'])):\n",
    "        ax1.text(step, percentage + 1, f'{percentage:.1f}%', \n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Simulation count by step\n",
    "    ax2.bar(summary_df['Step'], summary_df['Total_Simulations'], color='lightcoral', alpha=0.7)\n",
    "    ax2.set_xlabel('Step', fontsize=12)\n",
    "    ax2.set_ylabel('Number of Simulations', fontsize=12)\n",
    "    ax2.set_title('Simulation Count by Step', fontsize=14)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (step, count) in enumerate(zip(summary_df['Step'], summary_df['Total_Simulations'])):\n",
    "        ax2.text(step, count + max(summary_df['Total_Simulations']) * 0.01, \n",
    "                f'{count:,}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(threshold_dir, \"cross_step_summary.png\"), \n",
    "               dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1042313d-f7f6-453a-bea3-412747b8359c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsfit_dfs_stepwise[5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
